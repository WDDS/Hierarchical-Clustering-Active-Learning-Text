{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np \n",
    "import time\n",
    "from collections import Counter\n",
    "import re\n",
    "import math\n",
    "from gensim.models import KeyedVectors\n",
    "from pprint import pprint\n",
    "import pyspark\n",
    "\n",
    "MAX_LEAFS = 128\n",
    "\n",
    "class MeshCode:\n",
    "    def __init__(self, ID, name, treeNumber, child_mesh_code=[]):\n",
    "        self.id = ID\n",
    "        self.name = name\n",
    "        self.treeNumber = treeNumber\n",
    "        self.children = child_mesh_code\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return \"id: {}, name: {}\".format(self.id, self.name)\n",
    "    \n",
    "    \n",
    "MESH_HIERARCHY = MeshCode(\n",
    "    \"D003920\", \"Diabetes Mellitus\", \"C19.246\" , [\n",
    "        MeshCode(\"D048909\", \"Diabetes Complications\", \"C19.246.099\", [\n",
    "               MeshCode(\"D003925\", \"Diabetic Angiopathies\", \"C19.246.099.500\", [\n",
    "                       #MeshCode(\"D017719\", \"Diabetic Foot\", \"C19.246.099.500.191\")  # to prevent double foot \n",
    "                      MeshCode(\"D003930\", \"Diabetic Retinopathy\", \"C19.246.099.500.382\") \n",
    "               ]) \n",
    "              , MeshCode(\"D058065\", \"Diabetic Cardiomyopathies\", \"C19.246.099.625\") \n",
    "              , MeshCode(\"D003926\", \"Diabetic Coma\", \"C19.246.099.750\", [\n",
    "                       MeshCode(\"D006944\", \"Hyperglycemic Hyperosmolar Nonketotic Coma\", \"C19.246.099.750.490\", []) \n",
    "               ]) \n",
    "              , MeshCode(\"D016883\", \"Diabetic Ketoacidosis\", \"C19.246.099.812\") \n",
    "              , MeshCode(\"D003928\", \"Diabetic Nephropathies\", \"C19.246.099.875\") \n",
    "              , MeshCode(\"D003929\", \"Diabetic Neuropathies\", \"C19.246.099.937\", [\n",
    "                       MeshCode(\"D017719\", \"Diabetic Foot\", \"C19.246.099.937.250\") \n",
    "               ]) \n",
    "              , MeshCode(\"D005320\", \"Fetal Macrosomia\", \"C19.246.099.968\") \n",
    "        ])\n",
    "       , MeshCode(\"D016640\", \"Diabetes, Gestational\", \"C19.246.200\")\n",
    "       , MeshCode(\"D003921\", \"Diabetes Mellitus, Experimental\", \"C19.246.240\")\n",
    "       , MeshCode(\"D003922\", \"Diabetes Mellitus, Type 1\", \"C19.246.267\", [\n",
    "                MeshCode(\"D014929\", \"Wolfram Syndrome\", \"C19.246.267.960\")\n",
    "        ])\n",
    "       , MeshCode(\"D003924\", \"Diabetes Mellitus, Type 2\", \"C19.246.300\", [\n",
    "                MeshCode(\"D003923\", \"Diabetes Mellitus, Lipoatrophic\", \"C19.246.300.500\")\n",
    "        ])\n",
    "       , MeshCode(\"D056731\", \"Donohue Syndrome\", \"C19.246.537\")\n",
    "       , MeshCode(\"D000071698\", \"Latent Autoimmune Diabetes in Adults\", \"C19.246.656\")\n",
    "       , MeshCode(\"D011236\", \"Prediabetic State\", \"C19.246.774\")\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "class Tree(object):\n",
    "    \n",
    "    def __init__(self, tree_hierarchy, clusters_predict=[], mode=\"sklearn\", sentences_all_classes=None, true_classes_all=None):\n",
    "        \"\"\"\n",
    "        @param mode : Two possible values\n",
    "            - \"FBE\" : Tree object for Feedback Explorer output\n",
    "            - \"sklearn\" : Tree object for scikit learn output\n",
    "        \n",
    "        @param sentences_all_classes : List of all possible classes occuring in the sentences file (only for mode FBE)\n",
    "        @param true_labels_all : All occuring true labels (mesh codes) of the documents/abstracts\n",
    "        \"\"\"\n",
    "        self.tree = None\n",
    "        if mode in [\"sklearn\", \"FBE\"]:\n",
    "            self.mode = mode\n",
    "        else:\n",
    "            raise ValueError(\"Provided mode '{}' is not supported\".format(mode))\n",
    "        self.tree_hierarchy = tree_hierarchy # pandas dataframe with tree structure coming from hierarchical clustering\n",
    "        self.n_nodes = 0 # updated by calling self.count_nodes()\n",
    "        self.n_leafs = 0 # updated by calling self.count_leafs()\n",
    "        self.temp_n_leafs = 1 # In mode 'FBE' helps to construct the tree with the right number of nodes\n",
    "        self.clusters_predict = clusters_predict # predicted cluster for each document\n",
    "        self.unique_cluster_predict = list(set(clusters_predict)) # list of all classes to calculate performance metrices\n",
    "        self.leaf_nodes = [] # list of all leaf nodes\n",
    "        self.sentences_all_classes = sentences_all_classes # List of all classes occuring in sentences (phrases.parquet)\n",
    "        self.true_classes_documents = true_classes_all.values.tolist() # list of true labels (mesh codes) in the abstracts\n",
    "        self.true_classes_documents_unique = list(set(true_classes_all)) # all possible occuring true labels (mesh codes) in the abstracts\n",
    "        self.precision_all_nodes = [] # macro\n",
    "        self.precision_all_nodes_weighted = []\n",
    "        self.precision_all_nodes_weights = 0\n",
    "        self.precision_macro = None \n",
    "        self.precision_micro = None\n",
    "        self.recall_all_classes = []\n",
    "        self.recall_all_classes_weighted = []\n",
    "        self.recall_macro = None\n",
    "        self.recall_micro = None\n",
    "        self.F1_macro = None\n",
    "        self.F1_micro = None\n",
    "        self.maxDepth = 0\n",
    "        self.temp_max_occ_class_in_cluster = 0\n",
    "        self.temp_max_doc_perClass_inCluster = 0\n",
    "        self.temp_mesh_and_its_childs = [] # list of a given mesh code and its children mesh codes\n",
    "        \n",
    "        \n",
    "    def _build_tree(self, node, current_depth=None):\n",
    "        if self.mode == \"sklearn\":\n",
    "            if node.node_id in self.tree_hierarchy[\"node_id\"].values: # if node not leaf\n",
    "                treeChildren = self.tree_hierarchy[self.tree_hierarchy[\"node_id\"] == node.node_id]\n",
    "                node.add_child(Node(Id=treeChildren[\"left\"].values[0], depth=node.depth + 1, parent=node))\n",
    "                node.add_child(Node(Id=treeChildren[\"right\"].values[0], depth=node.depth + 1, parent=node))\n",
    "                self._build_tree(node.children[0])\n",
    "                self._build_tree(node.children[1])\n",
    "            else:\n",
    "                return node\n",
    "            return node\n",
    "        elif self.mode == \"FBE\": \n",
    "            # Only create node if node is in current depth level\n",
    "            if node.depth == current_depth and self.temp_n_leafs < MAX_LEAFS:\n",
    "                treeChildren = self.tree_hierarchy.iloc[node.node_id].children\n",
    "                #print(\"\\t{}\".format(node))\n",
    "                #print(\"tree children:\")\n",
    "                #print(treeChildren)\n",
    "                #print()\n",
    "                # FBE tree is not a perfect binary tree, some nodes don't create children any more\n",
    "                if len(treeChildren) > 0:\n",
    "                    cluster_child_one = self.tree_hierarchy.iloc[treeChildren[0]].filterValue[0]\n",
    "                    cluster_child_two = self.tree_hierarchy.iloc[treeChildren[1]].filterValue[0]\n",
    "                #    print(\"c1: {}, c2: {}\".format(cluster_child_one, cluster_child_two))\n",
    "                    # Some nodes from nodes.json are empty: no sentences is going through them\n",
    "                    # Only create node in tree when there is a sentence running through it\n",
    "                    if cluster_child_one in self.sentences_all_classes:\n",
    "                #        print(\"\\c1 in class\")\n",
    "                        self.temp_n_leafs -= 1 # lose one leaf because it is split into two new leafs\n",
    "                        node.add_child(Node(Id=treeChildren[0], depth=node.depth + 1, parent=node, cluster_label=cluster_child_one))\n",
    "                        self.temp_n_leafs += 1\n",
    "                        if cluster_child_two in sentences_all_classes:\n",
    "                #            print(\"\\tc1 and c2 in class\")\n",
    "                            node.add_child(Node(Id=treeChildren[1], depth=node.depth + 1, parent=node, cluster_label=cluster_child_two))\n",
    "                            self.temp_n_leafs += 1\n",
    "                    elif cluster_child_two in sentences_all_classes:\n",
    "                #        print(\"\\tc2 in class\")\n",
    "                        self.temp_n_leafs -= 1 # lose one leaf because it is split into two new leafs\n",
    "                        node.add_child(Node(Id=treeChildren[1], depth=node.depth + 1, parent=node, cluster_label=cluster_child_two))\n",
    "                        self.temp_n_leafs += 1          \n",
    "                #    else:\n",
    "                #        print(\"\\tno class for c1 and c2\")\n",
    "            else:\n",
    "                if len(node.children) == 1 and self.temp_n_leafs < MAX_LEAFS:\n",
    "                    self._build_tree(node.children[0], current_depth)\n",
    "                elif len(node.children) == 2 and self.temp_n_leafs < MAX_LEAFS:\n",
    "                    self._build_tree(node.children[0], current_depth)\n",
    "                    self._build_tree(node.children[1], current_depth)\n",
    "            return node\n",
    "\n",
    "    def _update_leaf_to_root(self, node, abstract_id, class_predict):\n",
    "        \"\"\" Updates node and all its ancestors up to the root with the abstract's id and the predicted class\"\"\"\n",
    "        node.update_node(abstract_id, class_predict)\n",
    "        if node.parent != None: # Root has no parent\n",
    "            self._update_leaf_to_root(node.parent, abstract_id, class_predict)\n",
    "    \n",
    "\n",
    "    def set_build_tree(self,node):\n",
    "        \"\"\" Builds the tree and sets the variable tree.\"\"\"  \n",
    "\n",
    "        # tree with MAX_LEAFS leafs is constructed. \n",
    "        # For sklearn add to each leaf its cluster label based on the children in the tree object from sklearn AgglomerativeClustering\n",
    "        self.leaf_nodes = []\n",
    "        if self.mode == \"sklearn\":\n",
    "            tree = self._build_tree(node) # construct whole tree\n",
    "            tree = self._get_cluster_labels_for_leafs(tree) # get labels for leafs\n",
    "            tree = self._cut_nodes_from_leafs(tree) # cut nodes from bottom of the tree until only leafs with a unique cluster_label exist (Number leaves = MAX_LEAFS)\n",
    "        elif self.mode == \"FBE\":\n",
    "            self.temp_n_leafs = 1\n",
    "            self.maxDepth = 0\n",
    "            self._get_maxDepth(0, 0)\n",
    "            depth = 0\n",
    "            #print(\"maxDepth: {}\".format(self.maxDepth))\n",
    "            # build tree by level: create first all children for level 1, then level 2... \n",
    "            # Prevents that a tree creates children just in one branch and always goes deeper in case of a max number of leavese\n",
    "            while self.temp_n_leafs < MAX_LEAFS and depth <= self.maxDepth:\n",
    "            #    print(\"\\n\\ndepth: {}, temp_n_leafs: {}\".format(depth, self.temp_n_leafs))\n",
    "                tree = self._build_tree(node, depth)\n",
    "                depth += 1\n",
    "\n",
    "        assert isinstance(tree, Node)\n",
    "        self.tree = tree\n",
    "        print(\"Count nodes: {}; leafs: {}\".format(self.count_nodes(), self.count_leafs()))\n",
    "\n",
    "\n",
    "    def _get_maxDepth(self, i, depth):\n",
    "        \"\"\" get max depth of tree\"\"\"\n",
    "        if depth > self.maxDepth:\n",
    "            self.maxDepth = depth        \n",
    "        node = self.tree_hierarchy.iloc[i]\n",
    "        if len(node.children) == 1:\n",
    "            self._get_maxDepth(node.children[0], depth+1)\n",
    "        elif len(node.children) == 2:\n",
    "            self._get_maxDepth(node.children[0], depth+1)\n",
    "            self._get_maxDepth(node.children[1], depth+1)\n",
    "\n",
    "\n",
    "        \n",
    "    def _get_cluster_labels_for_leafs(self, node):\n",
    "        \"\"\" \n",
    "            Get's the cluster labels for each leafs using the cluster labels assigned by\n",
    "            the output of the sklearn agglomerative clustering algorithm.\n",
    "        \"\"\"        \n",
    "        if len(node.children) == 0: #leaf\n",
    "            cluster_label = self.clusters_predict[node.node_id]\n",
    "            node.set_clusterLabel(cluster_label)\n",
    "        else: # no leaf\n",
    "            self._get_cluster_labels_for_leafs(node.children[0])\n",
    "            self._get_cluster_labels_for_leafs(node.children[1])\n",
    "        return node\n",
    "    \n",
    "    def _cut_nodes_from_leafs(self, node):\n",
    "        \"\"\" \n",
    "            self.mode == sklearn:\n",
    "            Children of nodes, who are leafs and have the same cluster_label, are cut off\n",
    "            and the parent node takes the cluster label of its children.\n",
    "            This is done recursively until there are only leafs with unique cluster_labels \n",
    "            Number of leaves = MAX_LEAFS\n",
    "        \n",
    "            self.mode == FBE:\n",
    "            Towards the bottom of the tree, it may happen that a node has only child, which has only one child, \n",
    "            and this child also has only one child, etc. Several nodes following of each other with only one child.\n",
    "            In this case keep only the child C whose parent has two children and cut the child of C.\n",
    "        \"\"\"\n",
    "        if self.mode == \"sklearn\":\n",
    "            if len(node.children) > 0: \n",
    "                left_child = node.children[0]\n",
    "                right_child = node.children[1]\n",
    "                if left_child.cluster_label is None: # left child is not leaf \n",
    "                    self._cut_nodes_from_leafs(left_child)\n",
    "                if right_child.cluster_label is None: # right child is not leaf \n",
    "                    self._cut_nodes_from_leafs(right_child)\n",
    "\n",
    "                # should be updated now\n",
    "                left_child = node.children[0]\n",
    "                right_child = node.children[1]\n",
    "                if left_child.cluster_label == right_child.cluster_label and left_child.cluster_label is not None:\n",
    "                    node.children = []\n",
    "                    node.cluster_label = left_child.cluster_label\n",
    "                    return node\n",
    "        elif self.mode == \"FBE\":\n",
    "            if len(node.children) == 1: # node has only one child\n",
    "                temp = node\n",
    "                while len(temp.children) == 1: # check if several nodes following of each other have only one child two\n",
    "                    temp = temp.children[0]\n",
    "                if len(temp.children) == 2: # if at some point a node has two children, continue to search\n",
    "                    self._cut_nodes_from_leafs(temp.children[0])    \n",
    "                    self._cut_nodes_from_leafs(temp.children[1])\n",
    "                else: # if we reached a leaf, cut the node's children\n",
    "                    node.children = []\n",
    "            elif len(node.children) == 2:\n",
    "                self._cut_nodes_from_leafs(node.children[0])    \n",
    "                self._cut_nodes_from_leafs(node.children[1])             \n",
    "        return node    \n",
    "\n",
    "    \n",
    "        \n",
    "    def fitTree(self, node, data):\n",
    "        \"\"\" Updates all the nodes of the tree according to the clustering from bottom to top \"\"\"\n",
    "\n",
    "        assert isinstance(node, Node)\n",
    "        if len(node.children) > 0: # no leaf\n",
    "            for child in node.children:\n",
    "                self.fitTree(child, data)\n",
    "        else: # leaf\n",
    "            if self.mode == \"sklearn\": \n",
    "                leaf_cluster_label = node.cluster_label\n",
    "                abstract_hits = data[data[\"class_predict\"] == leaf_cluster_label]\n",
    "                for i, row in abstract_hits.iterrows():\n",
    "                    leaf_abstract_id = row.name\n",
    "                    leaf_abstract_class_true = row.mesh_ui_diab # true class \n",
    "                    self._update_leaf_to_root(node, leaf_abstract_id, leaf_abstract_class_true)\n",
    "            elif self.mode == \"FBE\": # several documents per leaf\n",
    "                leaf_cluster_label = node.cluster_label\n",
    "                abstract_hits = data[data[\"uniqueCluster\"] == leaf_cluster_label]\n",
    "                for i, row in abstract_hits.iterrows():\n",
    "                    leaf_abstract_id = row[\"id\"]\n",
    "                    leaf_abstract_class_true = row[\"mesh_ui_diab\"]\n",
    "                    self._update_leaf_to_root(node, leaf_abstract_id, leaf_abstract_class_true)\n",
    "            else: \n",
    "                print(\"ERROR: mode should be one of ['sklearn', 'FBE']\")\n",
    "        return node\n",
    "         \n",
    "            \n",
    "    def count_nodes(self, tree=None):\n",
    "        self.n_nodes = 0\n",
    "        def _walk_count_nodes(node):\n",
    "            self.n_nodes += 1\n",
    "            for child in node.children:\n",
    "                _walk_count_nodes(child)   \n",
    "                \n",
    "        if tree == None:\n",
    "            _walk_count_nodes(self.tree)\n",
    "        else:\n",
    "            _walk_count_nodes(tree)\n",
    "        return self.n_nodes\n",
    "\n",
    "                \n",
    "    def count_leafs(self, tree=None):\n",
    "\n",
    "        def _walk_count_leafs(node):\n",
    "            if node.children == []:\n",
    "                self.n_leafs += 1\n",
    "                self.leaf_nodes.append(node)\n",
    "            else:\n",
    "                for child in node.children:\n",
    "                    _walk_count_leafs(child)\n",
    "        \n",
    "        self.n_leafs = 0\n",
    "        self.leaf_nodes = []\n",
    "        if tree == None:\n",
    "            _walk_count_leafs(self.tree)\n",
    "        else:\n",
    "            _walk_count_leafs(tree)\n",
    "        return self.n_leafs\n",
    "    \n",
    "    \n",
    "    def get_leaf_nodes(self):\n",
    "        def _walk_leaf_nodes(node):\n",
    "            if node.children == []:\n",
    "                self.leaf_nodes.append(node)\n",
    "            else:\n",
    "                for child in node.children:\n",
    "                    _walk_leaf_nodes(child)\n",
    "        \n",
    "        self.leaf_nodes = []\n",
    "        _walk_leaf_nodes(self.tree)\n",
    "        return self.leaf_nodes\n",
    "    \n",
    "    def _walk_precision(self, node):\n",
    "        node_precision = node.get_precision()\n",
    "        self.precision_all_nodes.append(node_precision)\n",
    "        self.precision_all_nodes_weighted.append(node_precision * node.counts)\n",
    "        self.precision_all_nodes_weights += node.counts\n",
    "        for child in node.children:\n",
    "            self._walk_precision(child)\n",
    "            \n",
    "    def get_precision(self):\n",
    "        self.precision_all_nodes = []\n",
    "        self.precision_all_nodes_weighted = []\n",
    "        self.precision_all_nodes_weights = 0\n",
    "        self._walk_precision(self.tree)\n",
    "        self.precision_macro = np.mean(self.precision_all_nodes)\n",
    "        self.precision_micro = np.sum(self.precision_all_nodes_weighted) / self.precision_all_nodes_weights\n",
    "        return {\"prec_macro\" : self.precision_macro\n",
    "                , \"prec_micro\" : self.precision_micro}\n",
    "\n",
    "        \n",
    "    def get_recall(self):\n",
    "        \n",
    "        self.recall_all_classes = []\n",
    "        self.recall_all_classes_weighted = []\n",
    "        def _walk_recall(node, c):\n",
    "            \"\"\" Get cluster with max documents of class c in which class c is the majority class \"\"\"\n",
    "            class_counts = Counter(node.true_classes).most_common()\n",
    "            majority_classes = [c for c, occ in class_counts  if occ == class_counts[0][1]] # there can be several majority classes in a node\n",
    "            #majority_class = Counter(node.true_classes).most_common()[0][0]\n",
    "            occ = node.true_classes.count(c)\n",
    "            #print()\n",
    "            #print(node)\n",
    "            #print(\"\\t{}\".format(node.true_classes))\n",
    "            #print(\"\\tmajority_classe: {}, occ({}): {}\".format(majority_classes, c, occ))\n",
    "            if c in majority_classes and occ > self.temp_max_occ_class_in_cluster:\n",
    "                self.temp_max_occ_class_in_cluster = occ\n",
    "            #    print(\"\\t updatetemp_max_occ_class_in_cluster: {}\".format(self.temp_max_occ_class_in_cluster))\n",
    "            #if (occ > self.temp_max_occ_class_in_cluster \n",
    "            #    and (c in majority_classes or node.children == [])\n",
    "            #   ): # if we found a cluster with higher occ of documents for class c and the class c is the majority class in the cluster or leaf node\n",
    "            #    self.temp_max_occ_class_in_cluster = occ\n",
    "            #    print(\"\\tupdatetemp_max_occ_class_in_cluster: {}\".format(self.temp_max_occ_class_in_cluster))\n",
    "            \n",
    "            #if (occ > self.temp_max_occ_class_in_cluster and c in majority_classes):\n",
    "            # self.temp_max_occ_class_in_cluster = occ\n",
    "            #    print(\"\\MAJ: tupdatetemp_max_occ_class_in_cluster: {}\".format(self.temp_max_occ_class_in_cluster))\n",
    "            #elif (occ > self.temp_max_occ_class_in_cluster and node.children == []):\n",
    "            #    self.temp_max_occ_class_in_cluster = occ\n",
    "            #    print(\"\\tLEAF: updatetemp_max_occ_class_in_cluster: {}\".format(self.temp_max_occ_class_in_cluster))\n",
    "    \n",
    "            for child in node.children:\n",
    "                _walk_recall(child, c)\n",
    "        \n",
    "        weights_sum = 0\n",
    "        for c in self.true_classes_documents_unique:\n",
    "            N_c = self.true_classes_documents.count(c)\n",
    "            #print(\"\\nc: {}, N_c: {}\".format(c, N_c))\n",
    "            self.temp_max_occ_class_in_cluster = 0\n",
    "#            _walk_recall(self.tree, c)\n",
    "            # TODO: check if it is right!\n",
    "            # # start with children; otherwise recalls for all classes will be highest in root\n",
    "            _walk_recall(self.tree.children[0], c) \n",
    "            _walk_recall(self.tree.children[1], c)\n",
    "            recall = self.temp_max_occ_class_in_cluster / N_c\n",
    "            #print(\"c: {}, recall: {}\".format(c, recall))\n",
    "\n",
    "            self.recall_all_classes.append(recall) #len(self.unique_cluster_predict))\n",
    "            self.recall_all_classes_weighted.append(recall * N_c)\n",
    "            weights_sum += N_c\n",
    "        self.recall_macro = np.mean(self.recall_all_classes)\n",
    "        self.recall_micro = np.sum(self.recall_all_classes_weighted) / weights_sum\n",
    "        return {\"recall_macro\" : self.recall_macro\n",
    "                ,\"recall_micro\" : self.recall_micro}\n",
    "    \n",
    "    def get_F1(self):\n",
    "        precision = self.get_precision()\n",
    "        recall = self.get_recall()        \n",
    "        \n",
    "        self.F1_macro = 2*precision[\"prec_macro\"]*recall[\"recall_macro\"] / (precision[\"prec_macro\"] + recall[\"recall_macro\"])\n",
    "        self.F1_micro = 2*precision[\"prec_micro\"]*recall[\"recall_micro\"] / (precision[\"prec_micro\"] + recall[\"recall_micro\"])\n",
    "        return {\"F1_macro\":self.F1_macro\n",
    "               ,\"F1_micro\":self.F1_micro}\n",
    "\n",
    "\n",
    "    def _get_child_mesh_classes(self, meshId, currentMesh, foundMeshInHierarchy=False): \n",
    "        \"\"\" For a given meshId, get all its child meshId's from meshHierarchy \"\"\"\n",
    "        if meshId == currentMesh.id:\n",
    "            foundMeshInHierarchy = True\n",
    "        if foundMeshInHierarchy:\n",
    "            self.temp_mesh_and_its_child_classes.append(currentMesh)\n",
    "        for mesh_child in currentMesh.children:\n",
    "            self._get_child_mesh_classes(meshId, mesh_child, foundMeshInHierarchy)\n",
    "\n",
    "\n",
    "    def F1_zhao(self, evaluateOnlyOnLeafs=False):\n",
    "        \"\"\" F1 score like in Evaluation of Hierarchical Clustering Algorithms forDocument Datasets from Zhao & Karypis \"\"\"\n",
    "        \n",
    "        def _walk_F1_zhao(node, mesh_and_child_classes, N_c, evaluateOnlyOnLeafs):\n",
    "            \"\"\" \n",
    "                Calculates F1 Score for a given list of mesh codes and its children mesh_and_child_classes (N_c = total number of documents of class c) \n",
    "                evaluateOnlyOnLeafs [True, False] : calculate F1 score only on leafs or on all nodes\n",
    "            \"\"\"\n",
    "            #print(\"\\t{}\".format(node))\n",
    "            #print(\"abstracts in node:\")\n",
    "            #print(node.true_classes)\n",
    "            #for m in mesh_and_child_classes:\n",
    "            #    print(\"\\t\\t mesh: {}; count mesh in node: {}\".format(m, node.true_classes.count(m.id)))\n",
    "            class_count = np.sum([node.true_classes.count(m.id) for m in mesh_and_child_classes])# + node.true_classes.count(childs of class c)\n",
    "            prec =  class_count / node.counts\n",
    "            recall = class_count / N_c #+ all documents from all children of c\n",
    "            if prec > 1e-10 or recall > 1e-10: # if prec or recall == 0 \n",
    "                F1 = 2 * prec * recall / (prec+recall)\n",
    "            else:\n",
    "                F1 = 0\n",
    "            #print(\"\\tclass_count: {}, prec: {}, recall: {}, F1: {}\".format(class_count, prec, recall, F1))            \n",
    "            if F1 > self.temp_max_doc_perClass_inCluster:\n",
    "                self.temp_max_doc_perClass_inCluster = F1\n",
    "\n",
    "            if not evaluateOnlyOnLeafs:\n",
    "                for child in node.children:\n",
    "                    _walk_F1_zhao(child, mesh_and_child_classes, N_c, evaluateOnlyOnLeafs)        \n",
    "        \n",
    "        if evaluateOnlyOnLeafs:\n",
    "            leafs = self.get_leaf_nodes()\n",
    "        \n",
    "        FScore_sum = 0\n",
    "        for meshid in self.true_classes_documents_unique:\n",
    "            self.temp_mesh_and_its_child_classes = [] # reset \n",
    "            self._get_child_mesh_classes(meshid, MESH_HIERARCHY) \n",
    "            mesh_and_child_classes = self.temp_mesh_and_its_child_classes\n",
    "            #N_c = self.true_classes_documents.count(c)\n",
    "            N_c = np.sum([self.true_classes_documents.count(m.id) for m in mesh_and_child_classes]) #+ all documents from all children of c\n",
    "            N = len(self.true_classes_documents)\n",
    "            #print(\"\\nc: {}, N_c: {}, N: {}\".format(meshid, N_c, N, N_c/N))\n",
    "            #print(\"\\t, mesh_childs: {}\".format( mesh_and_child_classes))\n",
    "            self.temp_max_doc_perClass_inCluster = 0\n",
    "            if evaluateOnlyOnLeafs == False: # evaluate on all nodes\n",
    "                _walk_F1_zhao(self.tree.children[0], mesh_and_child_classes, N_c, evaluateOnlyOnLeafs) \n",
    "                _walk_F1_zhao(self.tree.children[1], mesh_and_child_classes, N_c, evaluateOnlyOnLeafs)\n",
    "            else: # only leafs\n",
    "                for leaf in leafs:\n",
    "                    _walk_F1_zhao(leaf, mesh_and_child_classes, N_c, evaluateOnlyOnLeafs)\n",
    "            #print(\"Best F1: {}\".format(self.temp_max_doc_perClass_inCluster))\n",
    "            FScore_sum += (N_c / N ) * self.temp_max_doc_perClass_inCluster\n",
    "            #print(\"Score: {}\".format((N_c / N ) * self.temp_max_doc_perClass_inCluster))\n",
    "            \n",
    "        return FScore_sum\n",
    "    \n",
    "    def get_isim(self, data):\n",
    "        \"\"\" Internal similarity \"\"\"\n",
    "        \n",
    "        I_sum = 0 \n",
    "        def _walk_isim(node):\n",
    "            \n",
    "            print(\"Node: {}\".format(node))\n",
    "            print(\"abstracts: {}\".node.abstracts)\n",
    "            for child in node.children:\n",
    "                _walk_isim(child)\n",
    "            \n",
    "\n",
    "    \n",
    "    def get_performances(self, evaluateOnlyOnLeafs=False):\n",
    "        precision = self.get_precision()\n",
    "        recall = self.get_recall()\n",
    "        F1 = self.get_F1()\n",
    "        return({\n",
    "            \"prec_micro\" : precision[\"prec_micro\"]\n",
    "            ,\"prec_macro\" : precision[\"prec_macro\"]            \n",
    "            ,\"recall_micro\" : recall[\"recall_micro\"]\n",
    "            ,\"recall_macro\" : recall[\"recall_macro\"]\n",
    "            ,\"F1_micro\" : F1[\"F1_micro\"]\n",
    "            ,\"F1_macro\" : F1[\"F1_macro\"]\n",
    "            ,\"F1_zhao\" : self.F1_zhao(evaluateOnlyOnLeafs=evaluateOnlyOnLeafs)\n",
    "        })\n",
    " \n",
    "\n",
    "\n",
    "class Node(object):\n",
    "    \"Generic tree node.\"\n",
    "    def __init__(self, Id, depth, parent=None, cluster_label=None, children=[]):\n",
    "        self.node_id = Id\n",
    "        self.parent = parent\n",
    "        self.children = []\n",
    "        self.depth = depth\n",
    "        self.cluster_label = cluster_label # In case FBE: this is the filterValue in the leafs\n",
    "        self.abstracts = [] # PMID's of abstracts \n",
    "        self.true_classes = [] # True classes for each abstract\n",
    "        self.counts = 0\n",
    "        self.recall = None\n",
    "        self.precision = None \n",
    "        self.F1 = None\n",
    "        if children is not None:\n",
    "            for child in children:\n",
    "                self.add_child(child)\n",
    "                \n",
    "    def __repr__(self):\n",
    "        return \"Node id: {} (depth: {}, cluster_label: {}, children: {})\".format(\n",
    "            self.node_id\n",
    "            , self.depth\n",
    "            , self.cluster_label\n",
    "            , [child.node_id for child in self.children])\n",
    "    \n",
    "    def add_child(self, node):\n",
    "        assert isinstance(node, Node)\n",
    "        self.children.append(node)\n",
    "        \n",
    "    def set_clusterLabel(self, clusterLabel):\n",
    "        self.cluster_label = clusterLabel\n",
    "        \n",
    "    def pretty_print(self, depth=0):\n",
    "        \n",
    "        if self.depth == depth: \n",
    "            print(\"Node: {}, Parent: {} (Depth: {}, counts: {}, cluster_label: {}) | Children: {}\".format(self.node_id, self.parent, self.depth, self.counts, self.cluster_label, self.children))\n",
    "            print(\"\\tAbstracts: {}\".format(Counter(self.abstracts)))\n",
    "            print(\"\\ttrue_classes: {}\".format(Counter(self.true_classes)))\n",
    "        else:\n",
    "            for child in self.children:\n",
    "                child.pretty_print(depth)\n",
    "            \n",
    "            \n",
    "    def update_node(self, abstract_id, true_class):\n",
    "        \"\"\" Updates the abstracts and its true class label running through this node \"\"\"\n",
    "        self.abstracts.append(abstract_id)\n",
    "        self.true_classes.append(true_class)\n",
    "        self.counts += 1\n",
    "        \n",
    "        \n",
    "    def get_precision(self):\n",
    "        count = Counter(self.true_classes)\n",
    "        mostFrequent = max(self.true_classes, key=count.get)\n",
    "        prec = self.true_classes.count(mostFrequent) / self.counts\n",
    "        return prec\n",
    "\n",
    "    def count_class_occurrence(self, c):\n",
    "        return self.true_classes.count(c)\n",
    "    \n",
    "\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tree nodes: (50910, 3)\n",
      "   node_id   left  right\n",
      "0    50911  34157  45500\n",
      "1    50912  18450  47237\n",
      "2    50913   2323  43884\n",
      "3    50914   1011  12815\n",
      "4    50915  11145  19489\n",
      "data size: (50911, 10)\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47]\n",
      "Diabetic Retinopathy                          5000\n",
      "Diabetes Mellitus, Type 1                     5000\n",
      "Diabetes Mellitus, Experimental               5000\n",
      "Diabetes Mellitus, Type 2                     5000\n",
      "Diabetes Complications                        5000\n",
      "Diabetic Nephropathies                        5000\n",
      "Diabetes, Gestational                         5000\n",
      "Diabetic Foot                                 4424\n",
      "Diabetic Neuropathies                         3662\n",
      "Diabetic Angiopathies                         3026\n",
      "Diabetic Ketoacidosis                         1308\n",
      "Fetal Macrosomia                              1282\n",
      "Prediabetic State                             1261\n",
      "Diabetic Cardiomyopathies                      386\n",
      "Wolfram Syndrome                               228\n",
      "Hyperglycemic Hyperosmolar Nonketotic Coma      97\n",
      "Diabetic Coma                                   97\n",
      "Diabetes Mellitus, Lipoatrophic                 85\n",
      "Donohue Syndrome                                39\n",
      "Latent Autoimmune Diabetes in Adults            16\n",
      "Name: mesh_mh_diab, dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PMID</th>\n",
       "      <th>title</th>\n",
       "      <th>abstract</th>\n",
       "      <th>date</th>\n",
       "      <th>mesh_ui</th>\n",
       "      <th>mesh_mh</th>\n",
       "      <th>mesh_ui_diab</th>\n",
       "      <th>mesh_mh_diab</th>\n",
       "      <th>title_abstract_prep</th>\n",
       "      <th>class_predict</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>28800712</td>\n",
       "      <td>Outcomes Achieved With Use of a Prefabricated ...</td>\n",
       "      <td>BACKGROUND\\nThe total contact cast (TCC) is co...</td>\n",
       "      <td>2017-10</td>\n",
       "      <td>D000328,D000367,D000368,D000369,D002370,D01533...</td>\n",
       "      <td>Adult,Age Factors,Aged,Aged, 80 and over,Casts...</td>\n",
       "      <td>D017719</td>\n",
       "      <td>Diabetic Foot</td>\n",
       "      <td>outcomes achieved with use of a prefabricated ...</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6989594</td>\n",
       "      <td>Investigation of insulin sensitivity in early ...</td>\n",
       "      <td>Twenty-three normal weight subjects without an...</td>\n",
       "      <td>1980-01</td>\n",
       "      <td>D001786,D001835,D005230,D005951,D006801,D00732...</td>\n",
       "      <td>Blood Glucose,Body Weight,Fatty Acids, Noneste...</td>\n",
       "      <td>D011236</td>\n",
       "      <td>Prediabetic State</td>\n",
       "      <td>investigation of insulin sensitivity in early ...</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       PMID                                              title  \\\n",
       "0  28800712  Outcomes Achieved With Use of a Prefabricated ...   \n",
       "1   6989594  Investigation of insulin sensitivity in early ...   \n",
       "\n",
       "                                            abstract     date  \\\n",
       "0  BACKGROUND\\nThe total contact cast (TCC) is co...  2017-10   \n",
       "1  Twenty-three normal weight subjects without an...  1980-01   \n",
       "\n",
       "                                             mesh_ui  \\\n",
       "0  D000328,D000367,D000368,D000369,D002370,D01533...   \n",
       "1  D001786,D001835,D005230,D005951,D006801,D00732...   \n",
       "\n",
       "                                             mesh_mh mesh_ui_diab  \\\n",
       "0  Adult,Age Factors,Aged,Aged, 80 and over,Casts...      D017719   \n",
       "1  Blood Glucose,Body Weight,Fatty Acids, Noneste...      D011236   \n",
       "\n",
       "        mesh_mh_diab                                title_abstract_prep  \\\n",
       "0      Diabetic Foot  outcomes achieved with use of a prefabricated ...   \n",
       "1  Prediabetic State  investigation of insulin sensitivity in early ...   \n",
       "\n",
       "   class_predict  \n",
       "0             10  \n",
       "1             10  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#data = pd.read_parquet(\"/home/adrian/workspace/Hierarchical-Clustering-Active-Learning-Text/outputs_03082020/diabetes_abstracts_HC_output.parquet\")\n",
    "#data = pd.read_parquet(\"/home/adrian/workspace/Hierarchical-Clustering-Active-Learning-Text/output_withoutRootClassDiabetesMellitus_stopWordRemoval_K48/diabetes_abstracts_HC_output.parquet\")\n",
    "data = pd.read_parquet(\"/home/adrian/workspace/Hierarchical-Clustering-Active-Learning-Text/output_withoutRootClassDiabetesMellitus_K48/diabetes_abstracts_HC_output.parquet\")\n",
    "#data.index = data.index.get_level_values(\"PMID\")\n",
    "data.reset_index(drop=True, inplace=True)\n",
    "#data[\"PMID\"] = pd.to_numeric(data[\"PMID\"])\n",
    "#HC_tree = pd.read_parquet('/home/adrian/workspace/Hierarchical-Clustering-Active-Learning-Text/outputs_03082020/diabetes_abstracts_tree_output.parquet')\n",
    "#HC_tree = pd.read_parquet('/home/adrian/workspace/Hierarchical-Clustering-Active-Learning-Text/output_withoutRootClassDiabetesMellitus_stopWordRemoval_K48/diabetes_abstracts_tree_output.parquet')\n",
    "HC_tree = pd.read_parquet('/home/adrian/workspace/Hierarchical-Clustering-Active-Learning-Text/output_withoutRootClassDiabetesMellitus_K48/diabetes_abstracts_tree_output.parquet')\n",
    "\n",
    "# Ex. 10 samples\n",
    "#data = pd.read_parquet(\"/home/adrian/workspace/Hierarchical-Clustering-Active-Learning-Text/diabetes_abstracts_HC_output_10Examples.parquet\")\n",
    "#HC_tree = pd.read_parquet('/home/adrian/workspace/Hierarchical-Clustering-Active-Learning-Text/diabetes_abstracts_tree_output_10Examples.parquet')\n",
    "\n",
    "# Ex. 30 samples\n",
    "#data = pd.read_parquet(\"/home/adrian/workspace/Hierarchical-Clustering-Active-Learning-Text/diabetes_abstracts_HC_output_30Examples.parquet\")\n",
    "#HC_tree = pd.read_parquet('/home/adrian/workspace/Hierarchical-Clustering-Active-Learning-Text/diabetes_abstracts_tree_output_30Examples.parquet')\n",
    "\n",
    "## TEST TREE\n",
    "#data = pd.DataFrame({\"PMID\": [0, 1, 2, 3, 4, 5]\n",
    "#                    , \"class_predict\": [3, 0, 0, 0, 1, 2]}\n",
    "#                   , columns=[\"PMID\", \"class_predict\"]).set_index(\"PMID\")\n",
    "\n",
    "#HC_tree = pd.DataFrame({\"node_id\":[6, 7, 8, 9, 10]\n",
    "#                    , \"left\" : [1, 2, 0, 5, 8]\n",
    "#                    , \"right\" :[3, 6, 4, 7, 9]}\n",
    "#                   , columns=[\"node_id\", \"left\", \"right\"])\n",
    "\n",
    "print(\"Tree nodes: {}\".format(HC_tree.shape))\n",
    "print(HC_tree.head())\n",
    "print(\"data size: {}\".format(data.shape))\n",
    "print(list(set(data[\"class_predict\"])))\n",
    "#print(list(set(data[\"mesh_ui_diab\"])))\n",
    "#print(data[\"mesh_ui_diab\"])\n",
    "\n",
    "#df_vec = (data.title + \" \" + data.abstract).map(lambda abstract: avg_feature_vector(abstract))\n",
    "#print(type(df_vec))\n",
    "#df_vec = np.stack(df_vec.values, axis = 0)\n",
    "#print(df_vec.shape)\n",
    "#print(type(df_vec))\n",
    "\n",
    "print(data.mesh_mh_diab.value_counts())\n",
    "data.head(2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-99-64c3e1d77ba0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0many\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchar\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misdigit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mchar\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputString\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0;32mdef\u001b[0m \u001b[0mavg_feature_vector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_features\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex2word_set\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mindex2word_set\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m     \u001b[0;31m#words = sentence.split()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "# ADD vectors to abstracts\n",
    "\n",
    "#model = KeyedVectors.load_word2vec_format(\"/home/adrian/PhD/Data/Word2Vec/BioASQvectors2018/pubmed2018_w2v_200D/pubmed2018_w2v_200D.bin\", binary=True)\n",
    "#index2word_set = set(model.wv.index2word)\n",
    "# clean for BioASQ\n",
    "#bioclean = lambda t: re.sub('[.,?;*!%^&_+():-\\[\\]{}]', '', t.replace('\"', '').replace('/', '').replace('\\\\', '').replace(\"'\",'').strip().lower()).split()\n",
    "\n",
    "def hasNumbers(inputString):\n",
    "    return any(char.isdigit() for char in inputString)\n",
    "\n",
    "def avg_feature_vector(sentence, model=model, num_features=200, index2word_set=index2word_set):\n",
    "    #words = sentence.split()\n",
    "    try:\n",
    "        words = bioclean(sentence)\n",
    "    except:\n",
    "        print(\"bioclean did not work for: {}\".format(sentence))\n",
    "        print(type(sentence))\n",
    "        print(math.isnan(sentence))\n",
    "    feature_vec = np.zeros((num_features, ), dtype='float32')\n",
    "    n_words = 0\n",
    "    for word in words:\n",
    "        if word in index2word_set:\n",
    "            n_words += 1\n",
    "            feature_vec = np.add(feature_vec, model[word])\n",
    "        else:\n",
    "            if hasNumbers(word):\n",
    "                print(\"word not in vocabulary: {}\".format(word))\n",
    "    if (n_words > 0):\n",
    "        feature_vec = np.divide(feature_vec, n_words)\n",
    "    return feature_vec\n",
    "\n",
    "#data[\"vec\"] = (data.title + \" \" + data.abstract).map(lambda abstract: avg_feature_vector(abstract))\n",
    "#data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count nodes: 95; leafs: 48\n",
      "N nodes: 95\n",
      "N leafs: 48\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Node id: 98267 (depth: 2, cluster_label: 35, children: []),\n",
       " Node id: 28821 (depth: 3, cluster_label: 26, children: []),\n",
       " Node id: 100873 (depth: 3, cluster_label: 23, children: []),\n",
       " Node id: 48342 (depth: 2, cluster_label: 28, children: []),\n",
       " Node id: 12265 (depth: 3, cluster_label: 31, children: []),\n",
       " Node id: 101634 (depth: 5, cluster_label: 34, children: []),\n",
       " Node id: 101557 (depth: 6, cluster_label: 45, children: []),\n",
       " Node id: 101725 (depth: 6, cluster_label: 12, children: []),\n",
       " Node id: 23755 (depth: 5, cluster_label: 24, children: []),\n",
       " Node id: 18883 (depth: 7, cluster_label: 47, children: []),\n",
       " Node id: 101525 (depth: 7, cluster_label: 37, children: []),\n",
       " Node id: 101749 (depth: 7, cluster_label: 15, children: []),\n",
       " Node id: 90529 (depth: 8, cluster_label: 39, children: []),\n",
       " Node id: 78360 (depth: 10, cluster_label: 25, children: []),\n",
       " Node id: 101329 (depth: 11, cluster_label: 36, children: []),\n",
       " Node id: 101746 (depth: 11, cluster_label: 14, children: []),\n",
       " Node id: 101627 (depth: 11, cluster_label: 20, children: []),\n",
       " Node id: 24231 (depth: 12, cluster_label: 27, children: []),\n",
       " Node id: 101712 (depth: 12, cluster_label: 17, children: []),\n",
       " Node id: 101756 (depth: 11, cluster_label: 6, children: []),\n",
       " Node id: 101673 (depth: 13, cluster_label: 19, children: []),\n",
       " Node id: 101734 (depth: 13, cluster_label: 30, children: []),\n",
       " Node id: 101663 (depth: 14, cluster_label: 22, children: []),\n",
       " Node id: 99079 (depth: 15, cluster_label: 40, children: []),\n",
       " Node id: 101226 (depth: 15, cluster_label: 38, children: []),\n",
       " Node id: 27734 (depth: 15, cluster_label: 29, children: []),\n",
       " Node id: 18059 (depth: 16, cluster_label: 44, children: []),\n",
       " Node id: 101706 (depth: 16, cluster_label: 9, children: []),\n",
       " Node id: 30057 (depth: 16, cluster_label: 41, children: []),\n",
       " Node id: 101637 (depth: 16, cluster_label: 32, children: []),\n",
       " Node id: 101764 (depth: 18, cluster_label: 7, children: []),\n",
       " Node id: 101659 (depth: 19, cluster_label: 46, children: []),\n",
       " Node id: 101771 (depth: 19, cluster_label: 4, children: []),\n",
       " Node id: 101739 (depth: 18, cluster_label: 5, children: []),\n",
       " Node id: 99419 (depth: 19, cluster_label: 13, children: []),\n",
       " Node id: 101598 (depth: 21, cluster_label: 33, children: []),\n",
       " Node id: 101773 (depth: 21, cluster_label: 0, children: []),\n",
       " Node id: 101750 (depth: 22, cluster_label: 18, children: []),\n",
       " Node id: 101769 (depth: 22, cluster_label: 2, children: []),\n",
       " Node id: 28558 (depth: 23, cluster_label: 42, children: []),\n",
       " Node id: 101657 (depth: 23, cluster_label: 43, children: []),\n",
       " Node id: 101754 (depth: 23, cluster_label: 8, children: []),\n",
       " Node id: 101770 (depth: 24, cluster_label: 10, children: []),\n",
       " Node id: 101717 (depth: 25, cluster_label: 11, children: []),\n",
       " Node id: 101768 (depth: 25, cluster_label: 3, children: []),\n",
       " Node id: 101736 (depth: 17, cluster_label: 21, children: []),\n",
       " Node id: 101755 (depth: 18, cluster_label: 16, children: []),\n",
       " Node id: 101772 (depth: 18, cluster_label: 1, children: [])]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# initialise\n",
    "#MAX_LEAFS=8\n",
    "treeClass = Tree(HC_tree, data[\"class_predict\"], mode=\"sklearn\", true_classes_all=data[\"mesh_ui_diab\"])\n",
    "\n",
    "# define root node\n",
    "root = Node(Id=HC_tree[\"node_id\"].max() # In scikit learn, the root node is the one with maximum node id\n",
    "          , depth=0\n",
    "          , parent=None\n",
    "          , children=[])\n",
    "\n",
    "# build tree\n",
    "treeClass.set_build_tree(root)\n",
    "\n",
    "print(\"N nodes: {}\".format(treeClass.count_nodes()))\n",
    "print(\"N leafs: {}\".format(treeClass.count_leafs()))\n",
    "\n",
    "treeClass.leaf_nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit tree with abstracts \n",
    "tree_fit = treeClass.fitTree(treeClass.tree, data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#treeClass.tree.pretty_print(depth=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluate only leafs: True\n",
      "F1 score zhao: 0.6253834269576436\n"
     ]
    }
   ],
   "source": [
    "#print(treeClass.get_precision())\n",
    "#print(treeClass.get_recall())\n",
    "#print(treeClass.get_F1())\n",
    "#pprint(treeClass.get_performances())\n",
    "evaluateonlyleafs=True\n",
    "print(\"evaluate only leafs: {}\".format(evaluateonlyleafs))\n",
    "#pprint(treeClass.get_performances(evaluateOnlyOnLeafs=evaluateonlyleafs))\n",
    "print(\"F1 score zhao: {}\".format(treeClass.F1_zhao(evaluateOnlyOnLeafs=evaluateonlyleafs)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum([3,4,5])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# load FeedbackExplorer output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "515\n",
      "5000\n",
      "+--------+--------------------+--------------------+\n",
      "|      id|              tokens|               index|\n",
      "+--------+--------------------+--------------------+\n",
      "|28800712|[outcomes,  , ach...|[8 -> [442 -> 442...|\n",
      "| 6989594|[investigation,  ...|[164 -> [92 -> 92...|\n",
      "+--------+--------------------+--------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#fbe_path = \"/home/adrian/tmp/Test_FBE\"\n",
    "#fbe_path = \"/home/adrian/workspace/FBE output/maxTopwords_6_maxClasses1024_Nall\"\n",
    "#fbe_path = \"/home/adrian/workspace/FBE output_without_root_class_diabetesMellitus/maxTopwords_6_Nall\"\n",
    "MAX_LEAFS = 32\n",
    "\n",
    "fbe_path = \"/home/adrian/workspace/FBE output_without_root_class_diabetesMellitus/maxTopwords_6_N5000_affectOnlyHighScoreTokens_tryAsPoint_option0\"\n",
    "\n",
    "\n",
    "spark = pyspark.sql.SparkSession.builder.getOrCreate()\n",
    "sentences = spark.read.load(fbe_path+\"/phrases/\")\n",
    "print(len(sentences.columns))\n",
    "print(sentences.count())\n",
    "\n",
    "df_short = sentences.select(\"id\", \"tokens\", \"index\")\n",
    "#df_short.printSchema()\n",
    "df_short.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(256, 21)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>tagId</th>\n",
       "      <th>color</th>\n",
       "      <th>annotations</th>\n",
       "      <th>algo</th>\n",
       "      <th>strLinks</th>\n",
       "      <th>strClassPath</th>\n",
       "      <th>names</th>\n",
       "      <th>filterMode</th>\n",
       "      <th>filterValue</th>\n",
       "      <th>...</th>\n",
       "      <th>windowSize</th>\n",
       "      <th>classCenters</th>\n",
       "      <th>cError</th>\n",
       "      <th>childSplitSize</th>\n",
       "      <th>children</th>\n",
       "      <th>hits</th>\n",
       "      <th>metrics</th>\n",
       "      <th>rocCurve</th>\n",
       "      <th>externalClassesFreq</th>\n",
       "      <th>purity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>In Scope</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[{'tokens': ['aggregate'], 'tag': 1, 'from': N...</td>\n",
       "      <td>{'value': 'supervised'}</td>\n",
       "      <td>{'0': [1]}</td>\n",
       "      <td>{'1': [0]}</td>\n",
       "      <td>{}</td>\n",
       "      <td>{'value': 'allIn'}</td>\n",
       "      <td>[0]</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[1]</td>\n",
       "      <td>5000</td>\n",
       "      <td>{}</td>\n",
       "      <td>{}</td>\n",
       "      <td>{}</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Explorer</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[{'tokens': ['parentsand'], 'tag': 2, 'from': ...</td>\n",
       "      <td>{'value': 'clustering'}</td>\n",
       "      <td>{'1': [2, 3]}</td>\n",
       "      <td>{'2': [0, 1], '3': [0, 1]}</td>\n",
       "      <td>{}</td>\n",
       "      <td>{'value': 'anyIn'}</td>\n",
       "      <td>[1]</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{'2': 0, '3': 1}</td>\n",
       "      <td>[0.30347155211811905, 0.20720606913059703]</td>\n",
       "      <td>50.0</td>\n",
       "      <td>[2, 161]</td>\n",
       "      <td>5000</td>\n",
       "      <td>{}</td>\n",
       "      <td>{}</td>\n",
       "      <td>{}</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Explorer</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[{'tokens': ['predictionprevention'], 'tag': 4...</td>\n",
       "      <td>{'value': 'clustering'}</td>\n",
       "      <td>{'1': [4, 5]}</td>\n",
       "      <td>{'4': [0, 1, 2], '5': [0, 1, 2]}</td>\n",
       "      <td>{}</td>\n",
       "      <td>{'value': 'anyIn'}</td>\n",
       "      <td>[2]</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{'4': 0, '5': 1}</td>\n",
       "      <td>[0.30016739899043904, 0.220015458512949]</td>\n",
       "      <td>50.0</td>\n",
       "      <td>[3, 90]</td>\n",
       "      <td>3906</td>\n",
       "      <td>{}</td>\n",
       "      <td>{}</td>\n",
       "      <td>{}</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Explorer</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[{'tokens': ['diabetes'], 'tag': 8, 'from': No...</td>\n",
       "      <td>{'value': 'clustering'}</td>\n",
       "      <td>{'1': [8, 9]}</td>\n",
       "      <td>{'8': [0, 1, 2, 4], '9': [0, 1, 2, 4]}</td>\n",
       "      <td>{}</td>\n",
       "      <td>{'value': 'anyIn'}</td>\n",
       "      <td>[4]</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{'8': 0, '9': 1}</td>\n",
       "      <td>[0.290029150982945, 0.264503354541825]</td>\n",
       "      <td>50.0</td>\n",
       "      <td>[4, 61]</td>\n",
       "      <td>2980</td>\n",
       "      <td>{}</td>\n",
       "      <td>{}</td>\n",
       "      <td>{}</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Explorer</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[{'tokens': ['diabetes'], 'tag': 20, 'from': N...</td>\n",
       "      <td>{'value': 'clustering'}</td>\n",
       "      <td>{'1': [20, 21]}</td>\n",
       "      <td>{'20': [0, 1, 2, 8, 4], '21': [0, 1, 2, 8, 4]}</td>\n",
       "      <td>{}</td>\n",
       "      <td>{'value': 'anyIn'}</td>\n",
       "      <td>[8]</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{'20': 0, '21': 1}</td>\n",
       "      <td>[0.262124534816262, 0.28412328531006303]</td>\n",
       "      <td>50.0</td>\n",
       "      <td>[5, 40]</td>\n",
       "      <td>2832</td>\n",
       "      <td>{}</td>\n",
       "      <td>{}</td>\n",
       "      <td>{}</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows  21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       name  tagId  color                                        annotations  \\\n",
       "0  In Scope    0.0    NaN  [{'tokens': ['aggregate'], 'tag': 1, 'from': N...   \n",
       "1  Explorer    1.0    NaN  [{'tokens': ['parentsand'], 'tag': 2, 'from': ...   \n",
       "2  Explorer    NaN    NaN  [{'tokens': ['predictionprevention'], 'tag': 4...   \n",
       "3  Explorer    NaN    NaN  [{'tokens': ['diabetes'], 'tag': 8, 'from': No...   \n",
       "4  Explorer    NaN    NaN  [{'tokens': ['diabetes'], 'tag': 20, 'from': N...   \n",
       "\n",
       "                      algo         strLinks  \\\n",
       "0  {'value': 'supervised'}       {'0': [1]}   \n",
       "1  {'value': 'clustering'}    {'1': [2, 3]}   \n",
       "2  {'value': 'clustering'}    {'1': [4, 5]}   \n",
       "3  {'value': 'clustering'}    {'1': [8, 9]}   \n",
       "4  {'value': 'clustering'}  {'1': [20, 21]}   \n",
       "\n",
       "                                     strClassPath names          filterMode  \\\n",
       "0                                      {'1': [0]}    {}  {'value': 'allIn'}   \n",
       "1                      {'2': [0, 1], '3': [0, 1]}    {}  {'value': 'anyIn'}   \n",
       "2                {'4': [0, 1, 2], '5': [0, 1, 2]}    {}  {'value': 'anyIn'}   \n",
       "3          {'8': [0, 1, 2, 4], '9': [0, 1, 2, 4]}    {}  {'value': 'anyIn'}   \n",
       "4  {'20': [0, 1, 2, 8, 4], '21': [0, 1, 2, 8, 4]}    {}  {'value': 'anyIn'}   \n",
       "\n",
       "  filterValue  ...  windowSize        classCenters  \\\n",
       "0         [0]  ...         0.0                None   \n",
       "1         [1]  ...         NaN    {'2': 0, '3': 1}   \n",
       "2         [2]  ...         NaN    {'4': 0, '5': 1}   \n",
       "3         [4]  ...         NaN    {'8': 0, '9': 1}   \n",
       "4         [8]  ...         NaN  {'20': 0, '21': 1}   \n",
       "\n",
       "                                       cError childSplitSize  children  hits  \\\n",
       "0                                        None            NaN       [1]  5000   \n",
       "1  [0.30347155211811905, 0.20720606913059703]           50.0  [2, 161]  5000   \n",
       "2    [0.30016739899043904, 0.220015458512949]           50.0   [3, 90]  3906   \n",
       "3      [0.290029150982945, 0.264503354541825]           50.0   [4, 61]  2980   \n",
       "4    [0.262124534816262, 0.28412328531006303]           50.0   [5, 40]  2832   \n",
       "\n",
       "   metrics rocCurve externalClassesFreq purity  \n",
       "0       {}       {}                  {}     {}  \n",
       "1       {}       {}                  {}     {}  \n",
       "2       {}       {}                  {}     {}  \n",
       "3       {}       {}                  {}     {}  \n",
       "4       {}       {}                  {}     {}  \n",
       "\n",
       "[5 rows x 21 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nodes = pd.read_json(fbe_path+\"/nodes.json\", orient=\"records\")\n",
    "print(nodes.shape)\n",
    "nodes.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get list with all possible classes in the sentences file\n",
    "import pyspark\n",
    "from pyspark.sql.functions import udf, col\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "\n",
    "def get_list_all_possible_classes(sentences, data ):\n",
    "    \"\"\" Get the list of all possible occuring classes in the sentences file \"\"\"\n",
    "    join_udf = udf(lambda x: \";\".join(x))\n",
    "    sentences_classes_udf = udf(lambda x: \";\".join([str(v) for v in x.keys()]))\n",
    "\n",
    "    sentences_transformed = sentences.select(\"id\"\n",
    "                                            , \"tokens\"\n",
    "                                            , sentences_classes_udf('index').alias(\"all_classes\")) \\\n",
    "                                    .withColumn(\"tokens\", join_udf(col(\"tokens\"))) \n",
    "\n",
    "    sentences_pdf = sentences_transformed.toPandas()\n",
    "    sentences_pdf[\"id\"] = pd.to_numeric(sentences_pdf[\"id\"])\n",
    "\n",
    "    # add true class labels to sentences from data by merge/join \n",
    "    sentences_pdf[\"PMID\"] = sentences_pdf[\"id\"]\n",
    "    sentences_pdf[\"PMID\"] = pd.to_numeric(sentences_pdf[\"PMID\"])\n",
    "    meshDiab = data[[\"PMID\", \"mesh_ui_diab\"]]\n",
    "    meshDiab[\"PMID\"] = pd.to_numeric(meshDiab[\"PMID\"])\n",
    "    sentences_pd_with_classes = pd.merge(sentences_pdf, meshDiab, on='PMID', how=\"left\")\n",
    "\n",
    "    print(\"\\tsentences_pdf: {}\".format(sentences_pdf.shape))\n",
    "    print(\"\\tmeshDiab: {}\".format(meshDiab.shape))\n",
    "    print(\"\\tmerged: {}\".format(sentences_pd_with_classes.shape))\n",
    "\n",
    "    \n",
    "    # list of all classes in the sentences file\n",
    "    return (set(pd.to_numeric(sentences_pdf[\"all_classes\"].map(lambda sentence: sentence.split(\";\")).explode()).values)\n",
    "            , sentences_pd_with_classes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tsentences_pdf: (5000, 4)\n",
      "\tmeshDiab: (50911, 2)\n",
      "\tmerged: (5000, 5)\n",
      "Number of classes in sentences file: 238\n",
      "Merged dataset with true classes: (5000, 5)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/adrian/miniconda3/envs/deepscience/lib/python3.7/site-packages/ipykernel_launcher.py:24: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>tokens</th>\n",
       "      <th>all_classes</th>\n",
       "      <th>PMID</th>\n",
       "      <th>mesh_ui_diab</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>28800712</td>\n",
       "      <td>outcomes; ;achieved; ;with; ;use; ;of; ;a; ;pr...</td>\n",
       "      <td>0;1;2;66;51;420;4;485;21;8;31</td>\n",
       "      <td>28800712</td>\n",
       "      <td>D017719</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6989594</td>\n",
       "      <td>investigation; ;of; ;insulin; ;sensitivity; ;i...</td>\n",
       "      <td>0;1;2;164;20;4;439;8;44;28;429</td>\n",
       "      <td>6989594</td>\n",
       "      <td>D011236</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>524360</td>\n",
       "      <td>ultrastructural; ;pathology; ;of; ;peripheral;...</td>\n",
       "      <td>256;16;0;1;2;5;69;10;94</td>\n",
       "      <td>524360</td>\n",
       "      <td>D003929</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>21199315</td>\n",
       "      <td>evidence;-;based; ;interventional; ;pain; ;med...</td>\n",
       "      <td>0;1;2;66;51;420;4;485;21;8;31</td>\n",
       "      <td>21199315</td>\n",
       "      <td>D003929</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>24607755</td>\n",
       "      <td>delivery; ;timing; ;and; ;cesarean; ;delivery;...</td>\n",
       "      <td>0;1;2;435;20;4;167;8;443;28;45</td>\n",
       "      <td>24607755</td>\n",
       "      <td>D016640</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         id                                             tokens  \\\n",
       "0  28800712  outcomes; ;achieved; ;with; ;use; ;of; ;a; ;pr...   \n",
       "1   6989594  investigation; ;of; ;insulin; ;sensitivity; ;i...   \n",
       "2    524360  ultrastructural; ;pathology; ;of; ;peripheral;...   \n",
       "3  21199315  evidence;-;based; ;interventional; ;pain; ;med...   \n",
       "4  24607755  delivery; ;timing; ;and; ;cesarean; ;delivery;...   \n",
       "\n",
       "                      all_classes      PMID mesh_ui_diab  \n",
       "0   0;1;2;66;51;420;4;485;21;8;31  28800712      D017719  \n",
       "1  0;1;2;164;20;4;439;8;44;28;429   6989594      D011236  \n",
       "2         256;16;0;1;2;5;69;10;94    524360      D003929  \n",
       "3   0;1;2;66;51;420;4;485;21;8;31  21199315      D003929  \n",
       "4  0;1;2;435;20;4;167;8;443;28;45  24607755      D016640  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences_all_classes, sentences_pd_with_classes = get_list_all_possible_classes(sentences, data)\n",
    "print(\"Number of classes in sentences file: {}\".format(len(sentences_all_classes)))\n",
    "print(\"Merged dataset with true classes: {}\".format(sentences_pd_with_classes.shape))\n",
    "sentences_pd_with_classes.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count nodes: 68; leafs: 32\n",
      "Number leafs: 32\n"
     ]
    }
   ],
   "source": [
    "# initialise\n",
    "treeFBE = Tree(nodes\n",
    "            #, list(set(data[\"class_predict\"]))\n",
    "            , mode=\"FBE\"\n",
    "            , sentences_all_classes=sentences_all_classes\n",
    "            , true_classes_all=sentences_pd_with_classes[\"mesh_ui_diab\"])\n",
    "\n",
    "# define root node\n",
    "root = Node(Id=1, depth=0, parent=None, children=[]) # Id = 1 because start at Explorer \n",
    "\n",
    "# build tree\n",
    "#maxDepth = 10\n",
    "treeFBE.set_build_tree(root)\n",
    "\n",
    "print(\"Number leafs: {}\".format(treeFBE.count_leafs()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Associate cluster to each sentence\n",
    "from pyspark.sql.functions import isnan, when, count, col\n",
    "\n",
    "from pyspark.sql.functions import udf, col\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "def matchCluster(index_map, cluster): \n",
    "    \"\"\" gets for each abstract its unique cluster (filterValue) from the index\"\"\"\n",
    "    return list(set(list(index_map.keys())).intersection(set(cluster)))[0]\n",
    "\n",
    "def associate_unique_cluster_to_documents(sentences, tree):\n",
    "    \"\"\" Associates unique cluster to each document \"\"\"\n",
    "    leafs = tree.get_leaf_nodes()\n",
    "    print(\"N leafs: {}\".format(len(leafs)))\n",
    "    cluster = [leaf.cluster_label for leaf in leafs]\n",
    "    print(\"N clusters: {}\".format(len(set(cluster))))\n",
    "\n",
    "    matchCluster_udf = udf(lambda y: matchCluster(y, cluster))\n",
    "    join_udf = udf(lambda x: \";\".join(x))\n",
    "\n",
    "    sentences_transformed = sentences.select(\"id\", \"tokens\", matchCluster_udf('index').alias(\"uniqueCluster\")) \\\n",
    "                        .withColumn(\"tokens\", join_udf(col(\"tokens\"))) \n",
    "\n",
    "\n",
    "    sentences_transformed.select([count(when(isnan(c), c)).alias(c) for c in sentences_transformed.columns]).show()\n",
    "    #sentences.select('index', matchClass_udf('index').atlias(\"uniqueCluster\")).groupby(\"uniqueCluster\").count().show()\n",
    "    sentences_pd = sentences_transformed.toPandas()\n",
    "    sentences_pd[\"id\"] = pd.to_numeric(sentences_pd[\"id\"])\n",
    "    sentences_pd[\"uniqueCluster\"] = pd.to_numeric(sentences_pd[\"uniqueCluster\"])\n",
    "    \n",
    "    # add true class labels to data by merge/join \n",
    "    sentences_pd[\"PMID\"] = sentences_pd[\"id\"]\n",
    "    sentences_pd[\"PMID\"] = pd.to_numeric(sentences_pd[\"PMID\"])\n",
    "    meshDiab = data[[\"PMID\", \"mesh_ui_diab\"]]\n",
    "    meshDiab[\"PMID\"] = pd.to_numeric(meshDiab[\"PMID\"])\n",
    "    sentences_pd_with_classes_uniqueCluster = pd.merge(sentences_pd, meshDiab, on='PMID', how=\"left\")\n",
    "    #print(\"sentences_pd: {}\".format(sentences_pd.shape))\n",
    "    #print(\"meshDiab: {}\".format(meshDiab.shape))\n",
    "    #print(\"sentences_pd_with_classes_uniqueCluster: {}\".format(sentences_pd_with_classes.shape))\n",
    "    \n",
    "    return sentences_pd_with_classes_uniqueCluster \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N leafs: 32\n",
      "N clusters: 32\n"
     ]
    },
    {
     "ename": "PythonException",
     "evalue": "\n  An exception was thrown from Python worker in the executor. The below is the Python worker stacktrace.\nTraceback (most recent call last):\n  File \"/home/adrian/miniconda3/envs/deepscience/lib/python3.7/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 605, in main\n    process()\n  File \"/home/adrian/miniconda3/envs/deepscience/lib/python3.7/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 597, in process\n    serializer.dump_stream(out_iter, outfile)\n  File \"/home/adrian/miniconda3/envs/deepscience/lib/python3.7/site-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 223, in dump_stream\n    self.serializer.dump_stream(self._batched(iterator), stream)\n  File \"/home/adrian/miniconda3/envs/deepscience/lib/python3.7/site-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 141, in dump_stream\n    for obj in iterator:\n  File \"/home/adrian/miniconda3/envs/deepscience/lib/python3.7/site-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 212, in _batched\n    for item in iterator:\n  File \"/home/adrian/miniconda3/envs/deepscience/lib/python3.7/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 450, in mapper\n    result = tuple(f(*[a[o] for o in arg_offsets]) for (arg_offsets, f) in udfs)\n  File \"/home/adrian/miniconda3/envs/deepscience/lib/python3.7/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 450, in <genexpr>\n    result = tuple(f(*[a[o] for o in arg_offsets]) for (arg_offsets, f) in udfs)\n  File \"/home/adrian/miniconda3/envs/deepscience/lib/python3.7/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 90, in <lambda>\n    return lambda *a: f(*a)\n  File \"/home/adrian/miniconda3/envs/deepscience/lib/python3.7/site-packages/pyspark/python/lib/pyspark.zip/pyspark/util.py\", line 107, in wrapper\n    return f(*args, **kwargs)\n  File \"<ipython-input-8-109febaf9a1a>\", line 18, in <lambda>\n  File \"<ipython-input-8-109febaf9a1a>\", line 9, in matchCluster\nIndexError: list index out of range\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPythonException\u001b[0m                           Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-ef95c8f2602f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msentences_pd_with_classes_uniqueCluster\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0massociate_unique_cluster_to_documents\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtreeFBE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Unique clusters in sentences: {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentences_pd_with_classes_uniqueCluster\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"uniqueCluster\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnunique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#####\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0msentences_pd_with_classes_uniqueCluster\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-8-109febaf9a1a>\u001b[0m in \u001b[0;36massociate_unique_cluster_to_documents\u001b[0;34m(sentences, tree)\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m     \u001b[0msentences_transformed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwhen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0misnan\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0malias\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msentences_transformed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m     \u001b[0;31m#sentences.select('index', matchClass_udf('index').atlias(\"uniqueCluster\")).groupby(\"uniqueCluster\").count().show()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0msentences_pd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msentences_transformed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoPandas\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/deepscience/lib/python3.7/site-packages/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mshow\u001b[0;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[1;32m    438\u001b[0m         \"\"\"\n\u001b[1;32m    439\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtruncate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mtruncate\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 440\u001b[0;31m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshowString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvertical\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    441\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    442\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshowString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtruncate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvertical\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/deepscience/lib/python3.7/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1303\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1304\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1305\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1306\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1307\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/deepscience/lib/python3.7/site-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    135\u001b[0m                 \u001b[0;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m                 \u001b[0;31m# JVM exception message.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 137\u001b[0;31m                 \u001b[0mraise_from\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconverted\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    138\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/deepscience/lib/python3.7/site-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mraise_from\u001b[0;34m(e)\u001b[0m\n",
      "\u001b[0;31mPythonException\u001b[0m: \n  An exception was thrown from Python worker in the executor. The below is the Python worker stacktrace.\nTraceback (most recent call last):\n  File \"/home/adrian/miniconda3/envs/deepscience/lib/python3.7/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 605, in main\n    process()\n  File \"/home/adrian/miniconda3/envs/deepscience/lib/python3.7/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 597, in process\n    serializer.dump_stream(out_iter, outfile)\n  File \"/home/adrian/miniconda3/envs/deepscience/lib/python3.7/site-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 223, in dump_stream\n    self.serializer.dump_stream(self._batched(iterator), stream)\n  File \"/home/adrian/miniconda3/envs/deepscience/lib/python3.7/site-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 141, in dump_stream\n    for obj in iterator:\n  File \"/home/adrian/miniconda3/envs/deepscience/lib/python3.7/site-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 212, in _batched\n    for item in iterator:\n  File \"/home/adrian/miniconda3/envs/deepscience/lib/python3.7/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 450, in mapper\n    result = tuple(f(*[a[o] for o in arg_offsets]) for (arg_offsets, f) in udfs)\n  File \"/home/adrian/miniconda3/envs/deepscience/lib/python3.7/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 450, in <genexpr>\n    result = tuple(f(*[a[o] for o in arg_offsets]) for (arg_offsets, f) in udfs)\n  File \"/home/adrian/miniconda3/envs/deepscience/lib/python3.7/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 90, in <lambda>\n    return lambda *a: f(*a)\n  File \"/home/adrian/miniconda3/envs/deepscience/lib/python3.7/site-packages/pyspark/python/lib/pyspark.zip/pyspark/util.py\", line 107, in wrapper\n    return f(*args, **kwargs)\n  File \"<ipython-input-8-109febaf9a1a>\", line 18, in <lambda>\n  File \"<ipython-input-8-109febaf9a1a>\", line 9, in matchCluster\nIndexError: list index out of range\n"
     ]
    }
   ],
   "source": [
    "sentences_pd_with_classes_uniqueCluster = associate_unique_cluster_to_documents(sentences, treeFBE)\n",
    "print(\"Unique clusters in sentences: {}\".format(sentences_pd_with_classes_uniqueCluster[\"uniqueCluster\"].nunique())) #####\n",
    "\n",
    "sentences_pd_with_classes_uniqueCluster.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Node id: 1 (depth: 0, cluster_label: None, children: [2, 151])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "treeFBE.fitTree(treeFBE.tree, sentences_pd_with_classes_uniqueCluster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'prec_micro': 0.2099545142183972, 'prec_macro': 0.428851540150752, 'recall_micro': 0.5092, 'recall_macro': 0.28042934619697757, 'F1_micro': 0.29731813268585316, 'F1_macro': 0.3391112303598232, 'F1_zhao': 0.928483697139779}\n"
     ]
    }
   ],
   "source": [
    "#print(treeFBE.get_precision_macro())\n",
    "#print(treeFBE.get_recall_macro())\n",
    "#print(treeFBE.get_F1())\n",
    "print(treeFBE.get_performances())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performance FBE:\n",
      "{'F1_macro': 0.2506777108451753,\n",
      " 'F1_micro': 0.2435437686351443,\n",
      " 'F1_zhao': 0.6847747661652462,\n",
      " 'prec_macro': 0.32597387918191484,\n",
      " 'prec_micro': 0.17975198509950005,\n",
      " 'recall_macro': 0.2036393853315089,\n",
      " 'recall_micro': 0.3775215572273183}\n",
      "\n",
      "Performance scikit learn: \n",
      "{'F1_macro': 0.4313541702730686,\n",
      " 'F1_micro': 0.18398582428756732,\n",
      " 'F1_zhao': 0.7470408180623018,\n",
      " 'prec_macro': 0.5557336418367507,\n",
      " 'prec_micro': 0.10626421882440082,\n",
      " 'recall_macro': 0.35246787518737727,\n",
      " 'recall_micro': 0.6849796704052169}\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "print(\"Performance FBE:\")\n",
    "pprint(treeFBE.get_performances())\n",
    "print()\n",
    "print(\"Performance scikit learn: \")\n",
    "pprint(treeClass.get_performances())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----+\n",
      "|uniqueClass|count|\n",
      "+-----------+-----+\n",
      "|       1159|   21|\n",
      "|       1090|  234|\n",
      "|        296|   51|\n",
      "|        691|   33|\n",
      "|        125|    3|\n",
      "|        666|  256|\n",
      "|       1280|  334|\n",
      "|        124| 1199|\n",
      "|        718|  312|\n",
      "|        740| 1173|\n",
      "|        169|   41|\n",
      "|        747|   46|\n",
      "|       1425|   19|\n",
      "|        577|    5|\n",
      "|        272|   25|\n",
      "|         54|  968|\n",
      "|        282|    7|\n",
      "|        232|    1|\n",
      "|        483|   27|\n",
      "|       1158|    5|\n",
      "+-----------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sentences.select('index', matchClass_udf('index').alias(\"uniqueCluster\")).groupby(\"uniqueCluster\").count().show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate Performance for different FBE configurations. \n",
    "Limit number of abstracts to N; top words: 4,6,8 and other configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load data file: /home/adrian/workspace/FBE output_without_root_class_diabetesMellitus/maxTopwords_6_N5000_tryAsPoint_option0\n",
      "N sentences: 5000\n",
      "Load tree..\n",
      "nodes: (190, 21)\n",
      "Get list with all possible classes in the sentences file..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/adrian/miniconda3/envs/deepscience/lib/python3.7/site-packages/ipykernel_launcher.py:24: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tsentences_pdf: (5000, 4)\n",
      "\tmeshDiab: (50911, 2)\n",
      "\tmerged: (5000, 5)\n",
      "Number of classes in sentences file: 158\n",
      "Merged dataset with true classes: (5000, 5)\n",
      "initialise tree..\n",
      "Count nodes: 78; leafs: 32\n",
      "Associate cluster to each sentence..\n",
      "N leafs: 32\n",
      "N clusters: 32\n",
      "+---+------+-------------+\n",
      "| id|tokens|uniqueCluster|\n",
      "+---+------+-------------+\n",
      "|  0|     0|            0|\n",
      "+---+------+-------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/adrian/miniconda3/envs/deepscience/lib/python3.7/site-packages/ipykernel_launcher.py:35: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique clusters in sentences: 32\n",
      "Fit..\n",
      "evaluate only leafs: False\n",
      "{'F1_macro': 0.2283475803834915,\n",
      " 'F1_micro': 0.17171571625710172,\n",
      " 'F1_zhao': 0.6591743123046492,\n",
      " 'prec_macro': 0.5006499677522831,\n",
      " 'prec_micro': 0.1266880583577645,\n",
      " 'recall_macro': 0.14790330606781005,\n",
      " 'recall_micro': 0.2664}\n"
     ]
    }
   ],
   "source": [
    "MAX_LEAFS = 32\n",
    "\n",
    "#fbe_path = \"/home/adrian/workspace/FBE output_without_root_class_diabetesMellitus/maxTopwords_6_N5000\"\n",
    "#fbe_path = \"/home/adrian/workspace/FBE output_without_root_class_diabetesMellitus/maxTopwords_6_Nall\"\n",
    "#fbe_path = \"/home/adrian/workspace/FBE output_without_root_class_diabetesMellitus/maxTopwords_6_N5000_shuffleRepartition\"\n",
    "#fbe_path = \"/home/adrian/workspace/FBE output_without_root_class_diabetesMellitus/maxTopwords_6_N5000_affectonlyclassifhighestscore\"\n",
    "#fbe_path = \"/home/adrian/workspace/FBE output_without_root_class_diabetesMellitus/maxTopwords_6_N5000_affectOnlyHighScoreTokens_tryAsPoint_option0\"\n",
    "fbe_path = \"/home/adrian/workspace/FBE output_without_root_class_diabetesMellitus/maxTopwords_6_N5000_tryAsPoint_option0\"\n",
    "\n",
    "#fbe_path = \"/home/adrian/workspace/FBE output/maxTopwords_4_N5000\"\n",
    "#fbe_path = \"/home/adrian/workspace/FBE output/maxTopwords_6_Nall\"\n",
    "#fbe_path = \"/home/adrian/workspace/FBE output/maxTopwords_6_N5000\"\n",
    "#fbe_path = \"/home/adrian/workspace/FBE output/maxTopwords_4_N2000\"\n",
    "#fbe_path = \"/home/adrian/workspace/FBE output/maxTopwords_6_parallelism1_N5000\"\n",
    "#fbe_path = \"/home/adrian/workspace/FBE output/maxTopwords_6_N5000_shuffleRepartition\"\n",
    "\n",
    "print(\"Load data file: {}\".format(fbe_path))\n",
    "spark = pyspark.sql.SparkSession.builder.getOrCreate()\n",
    "sentences = spark.read.load(fbe_path+\"/phrases/\")\n",
    "df_short = sentences.select(\"id\", \"tokens\", \"index\")\n",
    "print(\"N sentences: {}\".format(sentences.count()))\n",
    "\n",
    "print(\"Load tree..\")\n",
    "nodes = pd.read_json(fbe_path+\"/nodes.json\", orient=\"records\")\n",
    "print(\"nodes: {}\".format(nodes.shape))\n",
    "\n",
    "print(\"Get list with all possible classes in the sentences file..\")\n",
    "sentences_all_classes, sentences_pd_with_classes = get_list_all_possible_classes(sentences, data)\n",
    "print(\"Number of classes in sentences file: {}\".format(len(sentences_all_classes)))\n",
    "print(\"Merged dataset with true classes: {}\".format(sentences_pd_with_classes.shape))\n",
    "\n",
    "print(\"initialise tree..\")\n",
    "treeFBE = Tree(nodes\n",
    "            , mode=\"FBE\"\n",
    "            , sentences_all_classes=sentences_all_classes\n",
    "            , true_classes_all=sentences_pd_with_classes[\"mesh_ui_diab\"])\n",
    "\n",
    "root = Node(Id=1, depth=0, parent=None, children=[]) # Id = 1 because start at Explorer \n",
    "treeFBE.set_build_tree(root)\n",
    "\n",
    "print(\"Associate cluster to each sentence..\")\n",
    "sentences_pd_with_classes_uniqueCluster = associate_unique_cluster_to_documents(sentences, treeFBE)\n",
    "print(\"Unique clusters in sentences: {}\".format(sentences_pd_with_classes_uniqueCluster[\"uniqueCluster\"].nunique())) #####\n",
    "\n",
    "print(\"Fit..\")\n",
    "treeFBE.fitTree(treeFBE.tree, sentences_pd_with_classes_uniqueCluster)\n",
    "evaluateonlyleafs=False\n",
    "print(\"evaluate only leafs: {}\".format(evaluateonlyleafs))\n",
    "pprint(treeFBE.get_performances(evaluateOnlyOnLeafs=evaluateonlyleafs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Performances for directories in : /home/adrian/workspace/FBE output\n",
    "### old metrics not taking the hierarchy of the mesh classes into consideration\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Performances for directories in : /home/adrian/workspace/FBE output_without_root_class_diabetesMellitus/\n",
    "### metrics taking the hierarchy of the mesh classes into consideration\n",
    "### excluded abstract having the root class diabetesMellitus\n",
    "\n",
    "Topwords 6; N = 5000 , N leafs: 32, MAX_LEAFS = 32 - 13-08-2020\n",
    "{'F1_macro': 0.2283475803834915,\n",
    " 'F1_micro': 0.17171571625710172,\n",
    " 'F1_zhao': 0.6591743123046493,\n",
    " 'prec_macro': 0.5006499677522831,\n",
    " 'prec_micro': 0.1266880583577645,\n",
    " 'recall_macro': 0.14790330606781005,\n",
    " 'recall_micro': 0.2664}\n",
    "\n",
    "Topwords 4; N = 5000, N_leafs 10, MAX_LEAFS = 32 - 13-08-2020\n",
    "{'F1_macro': 0.14595941787205505,\n",
    " 'F1_micro': 0.17322179714375205,\n",
    " 'F1_zhao': 0.6548679412028475,\n",
    " 'prec_macro': 0.37766171374779717,\n",
    " 'prec_micro': 0.1681580344278456,\n",
    " 'recall_macro': 0.09046035377974807,\n",
    " 'recall_micro': 0.1786}\n",
    "\n",
    "Topwords 8; N = 5000, N leafs: 32, MAX_LEAFS = 32 - 13-08-2020\n",
    "{'F1_macro': 0.22784874417496395,\n",
    " 'F1_micro': 0.2766414682309211,\n",
    " 'F1_zhao': 0.7375354163826922,\n",
    " 'prec_macro': 0.30292190695722104,\n",
    " 'prec_micro': 0.23253333333333334,\n",
    " 'recall_macro': 0.1825959691243019,\n",
    " 'recall_micro': 0.3414}\n",
    "\n",
    "Topwords 10; N = 5000, N leafs: 32, MAX_LEAFS = 32 - 13-08-2020\n",
    "{'F1_macro': 0.21299064343968985,\n",
    " 'F1_micro': 0.2603354180581699,\n",
    " 'F1_zhao': 0.6163431147102084,\n",
    " 'prec_macro': 0.2732769434400456,\n",
    " 'prec_micro': 0.21216666666666667,\n",
    " 'recall_macro': 0.17449594092020146,\n",
    " 'recall_micro': 0.3368}\n",
    "\n",
    "\n",
    "Topwords 6; N = 5000 ; parallelism 1, N leafs: 32, MAX_LEAFS = 32 - 13-08-2020\n",
    "{'F1_macro': 0.2283475803834915,\n",
    " 'F1_micro': 0.17171571625710172,\n",
    " 'F1_zhao': 0.6591743123046493,\n",
    " 'prec_macro': 0.5006499677522831,\n",
    " 'prec_micro': 0.1266880583577645,\n",
    " 'recall_macro': 0.14790330606781005,\n",
    " 'recall_micro': 0.2664}\n",
    "\n",
    "Topwords 6; N = 5000 ; N leafs: 32, MAX_LEAFS: 32, Shuffle+repartition  - 13-08-2020\n",
    "{'F1_macro': 0.18794525138269813,\n",
    " 'F1_micro': 0.21350317737968985,\n",
    " 'F1_zhao': 0.6500299953630889,\n",
    " 'prec_macro': 0.2794831542835792,\n",
    " 'prec_micro': 0.17513448894202033,\n",
    " 'recall_macro': 0.1415756078311793,\n",
    " 'recall_micro': 0.2734}\n",
    "\n",
    "Topwords 6; N = 5000 ; N leafs: 32, MAX_LEAFS: 32, tryAsPoint option 0 - 20-08-2020\n",
    "{'F1_macro': 0.2283475803834915,\n",
    " 'F1_micro': 0.17171571625710172,\n",
    " 'F1_zhao': 0.6591743123046492,\n",
    " 'prec_macro': 0.5006499677522831,\n",
    " 'prec_micro': 0.1266880583577645,\n",
    " 'recall_macro': 0.14790330606781005,\n",
    " 'recall_micro': 0.2664}\n",
    "\n",
    "\n",
    "Topwords 6; N = 5000 ; N leafs: 19, MAX_LEAFS: 32, affect class only if higher score - 13-08-2020\n",
    "{'F1_macro': 0.22375833207416565,\n",
    " 'F1_micro': 0.25032671503105836,\n",
    " 'F1_zhao': 0.7382670537548556,\n",
    " 'prec_macro': 0.3453166161736336,\n",
    " 'prec_micro': 0.20733993102414155,\n",
    " 'recall_macro': 0.1654993019057332,\n",
    " 'recall_micro': 0.3158}\n",
    "\n",
    "    Topwords 6; N = 5000 ; N leafs: 16, MAX_LEAFS: 16\n",
    "    F1 zhao: 0.6121491555899314\n",
    "    Topwords 6; N = 5000 ; N leafs: 16, MAX_LEAFS: 16, affect class only if higher score\n",
    "    F1 zhao: 0.6840435816852406\n",
    "        \n",
    "Topwords 6; N = 5000 ; N leafs: 32, MAX_LEAFS: 32, affect class only if higher score  , tryAsPoint option 1 20-08-2020  \n",
    "{'F1_macro': 0.21133315283887605,\n",
    " 'F1_micro': 0.23013195516973853,\n",
    " 'F1_zhao': 0.7340422296618938,\n",
    " 'prec_macro': 0.428851540150752,\n",
    " 'prec_micro': 0.2099545142183972,\n",
    " 'recall_macro': 0.14021467309848876,\n",
    " 'recall_micro': 0.2546}\n",
    "\n",
    "Topwords 6; N = 5000 ; N leafs: 32, MAX_LEAFS: 32, affect class only if higher score  , tryAsPoint option 3  - 20-08-2020  \n",
    "{'F1_macro': 0.26438731620281275,\n",
    " 'F1_micro': 0.2752757846646198,\n",
    " 'F1_zhao': 0.7260631440042862,\n",
    " 'prec_macro': 0.42729562484854666,\n",
    " 'prec_micro': 0.22132764765784113,\n",
    " 'recall_macro': 0.19141103111608776,\n",
    " 'recall_micro': 0.364}\n",
    "    \n",
    "Topwords 6; N = 5000 ; N leafs: 32, MAX_LEAFS = 32 only evaluation on leafs !!!!\n",
    "{'F1_macro': 0.2283475803834915,\n",
    " 'F1_micro': 0.17171571625710172,\n",
    " 'F1_zhao': 0.6099679983351846,\n",
    " 'prec_macro': 0.5006499677522831,\n",
    " 'prec_micro': 0.1266880583577645,\n",
    " 'recall_macro': 0.14790330606781005,\n",
    " 'recall_micro': 0.2664}\n",
    "\n",
    "\n",
    "Spark HC Bisecting Kmeans, N = 5000, F1_Zhao\n",
    "K=64 : 0.31938839779134753\n",
    "K=32 : 0.4867501249189368\n",
    "K=20 : 0.531422305528106\n",
    " \n",
    "\n",
    "####\n",
    "ALL TWEETS, F1_Zhao scores\n",
    "\n",
    "FBE \n",
    "K64 => leafs 64 : 0.6839356043600647\n",
    "K32 => leafs 32 : 0.6837704144246328\n",
    "K20 => leafs 20 : 0.6836459020792343\n",
    "    \n",
    "scikitlearn\n",
    "K128 => leafs 128 : 0.7470408180623019\n",
    "K64 => leafs 64 : 0.6325404570785932\n",
    "K48 => leafs 48 : 0.6323785789287327\n",
    "K32 => leafs 32 : 0.6319947764196064\n",
    "K20 => leafs 20 : 0.6319261108094082\n",
    "\n",
    "scikitlearn stop words removed\n",
    "K128 => leafs 128: 0.7905416892568666\n",
    "K64 => leafs 64 : 0.7797430966034341\n",
    "K48 => leafs 48 : 0.6335563625752151\n",
    "K32 => leafs 32 : 0.632090319013169\n",
    "K20 => leafs 20 : 0.6319716282425101\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "####\n",
    "ALL TWEETS, F1_Zhao scores only leafs K=32\n",
    "\n",
    "FBE \n",
    "K64 => leafs 64 : 0.47186540814593\n",
    "K32 => leafs 32 : 0.4923950663139429\n",
    "K20 => leafs 20 : 0.4976217764396761\n",
    "    \n",
    "spark\n",
    "K64 : 0.28628270701459085\n",
    "K32 : 0.4646152670187616\n",
    "K20 : 0.49505558398827937\n",
    "\n",
    "scikitlearn\n",
    "K128 => leafs 128 : 0.7196017046951705\n",
    "K64 : 0.6307555786658395\n",
    "K48 : 0.6309833346053897\n",
    "K32 : 0.6315189572838107\n",
    "K20 : 0.6315643667948513\n",
    "    \n",
    "scikitlearn stop words removed\n",
    "K128 => leafs 128 : 0.7241706562939704\n",
    "K64 => leafs 64 : 0.7171002919301714\n",
    "K48 => leafs 48 : 0.6253834269576436\n",
    "K32 => leafs 32 : 0.6309985009823309\n",
    "K20 => leafs 20 : 0.6315492474421875\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate Spark HC BisectingKmeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5000, 3)\n",
      "D003930       514\n",
      "D016640       498\n",
      "D003921       495\n",
      "D003924       493\n",
      "D003922       486\n",
      "D003928       486\n",
      "D048909       461\n",
      "D017719       444\n",
      "D003929       347\n",
      "D003925       290\n",
      "D011236       144\n",
      "D016883       131\n",
      "D005320       125\n",
      "D058065        29\n",
      "D014929        19\n",
      "D006944        13\n",
      "D003926        11\n",
      "D003923        10\n",
      "D056731         3\n",
      "D000071698      1\n",
      "Name: mesh_ui_diab, dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PMID</th>\n",
       "      <th>mesh_ui_diab</th>\n",
       "      <th>prediction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>28800712</td>\n",
       "      <td>D017719</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6989594</td>\n",
       "      <td>D011236</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       PMID mesh_ui_diab  prediction\n",
       "0  28800712      D017719           0\n",
       "1   6989594      D011236          13"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sparkHC = spark.read.parquet(\"/home/adrian/workspace/Spark_BisectingKmeans_without_root_class_diabetesMellitus/bisectingKmeans_out_K20_N5000.parquet\") # 256, 32, 21\n",
    "#sparkHC = spark.read.parquet(\"/home/adrian/workspace/Spark_BisectingKmeans/bisectingKmeans_out_K32_N5000.parquet\") # 256, 32, 21\n",
    "#sparkHC = spark.read.parquet(\"/home/adrian/workspace/Spark_BisectingKmeans/bisectingKmeans_out_K256.parquet\") # 256, 32, 21\n",
    "#sparkHC = spark.read.parquet(\"/home/adrian/workspace/Spark_BisectingKmeans/bisectingKmeans_out_K128.parquet\") # 256, 32, 21\n",
    "sparkHC_pd = sparkHC.select(\"PMID\", \"mesh_ui_diab\", \"prediction\").toPandas()\n",
    "print(sparkHC_pd.shape)\n",
    "print(sparkHC_pd[\"mesh_ui_diab\"].value_counts())\n",
    "sparkHC_pd.head(2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/adrian/miniconda3/envs/deepscience/lib/python3.7/site-packages/ipykernel_launcher.py:30: RuntimeWarning: invalid value encountered in long_scalars\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.531422305528106"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# taking class hierarchy into account\n",
    "\n",
    "def _get_child_mesh_classes(meshId, currentMesh, foundMeshInHierarchy=False): \n",
    "    \"\"\" For a given meshId, get all its child meshId's from meshHierarchy \"\"\"\n",
    "    global temp_mesh_and_its_child_classes\n",
    "    if meshId == currentMesh.id:\n",
    "        foundMeshInHierarchy = True\n",
    "    if foundMeshInHierarchy:\n",
    "        temp_mesh_and_its_child_classes.append(currentMesh)\n",
    "    for mesh_child in currentMesh.children:\n",
    "        _get_child_mesh_classes(meshId, mesh_child, foundMeshInHierarchy)\n",
    "\n",
    "\n",
    "true_classes_unique = list(set(sparkHC_pd[\"mesh_ui_diab\"].values.tolist()))\n",
    "\n",
    "N = sparkHC_pd.shape[0]\n",
    "\n",
    "FScore_sum = 0\n",
    "for meshid in true_classes_unique: # For each class class_count = np.sum([node.true_classes.count(m.id) for m in mesh_and_child_classes])# + node.true_classes.count(childs of class c)\n",
    "    temp_mesh_and_its_child_classes = [] # reset \n",
    "    _get_child_mesh_classes(meshid, MESH_HIERARCHY) \n",
    "    mesh_and_child_classes = temp_mesh_and_its_child_classes    \n",
    "    N_c = np.sum([(sparkHC_pd.mesh_ui_diab == m.id).sum() for m in mesh_and_child_classes]) #+ all documents from all children of c\n",
    "    #print(\"\\nc: {}, N_c: {}, N: {}\".format(meshid, N_c, N))\n",
    "    #print(\"\\t, mesh_childs: {}\".format( mesh_and_child_classes))    \n",
    "    temp_max_F1 = 0\n",
    "    for current_cluster in range(0, 32): # 21 # For each cluster\n",
    "        docs_for_current_cluster = sparkHC_pd[sparkHC_pd[\"prediction\"] == current_cluster]\n",
    "        class_count = np.sum([docs_for_current_cluster[docs_for_current_cluster[\"mesh_ui_diab\"] == m.id].shape[0] for m in mesh_and_child_classes])# + node.true_classes.count(childs of class c)\n",
    "        prec =  class_count / docs_for_current_cluster.shape[0] #node.counts\n",
    "        recall = class_count / N_c\n",
    "        #print(\"\\t\\t cluster: {} :: class_count: {}, self.count: {}, p: {:.2f}, r: {:.2f}\".format(current_cluster, class_count, docs_for_current_cluster.shape[0], prec, recall))\n",
    "        if prec > 1e-10 or recall > 1e-10: # if prec or recall == 0 \n",
    "            F1 = 2 * prec * recall / (prec+recall)\n",
    "        else:\n",
    "            F1 = 0\n",
    "\n",
    "        if F1 > temp_max_F1:\n",
    "            temp_max_F1 = F1        \n",
    "            #print(\"\\t\\tJIP\")\n",
    "    #print(\"Best F1: {}\".format(temp_max_F1))\n",
    "    #print(\"Nc/N + F1 : {}\".format((N_c / N ) * temp_max_F1))\n",
    "    FScore_sum += (N_c / N ) * temp_max_F1\n",
    "    \n",
    "FScore_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.26497261586518334"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# without taking class hierarchy into account\n",
    "true_classes_unique = list(set(sparkHC_pd[\"mesh_ui_diab\"].values.tolist()))\n",
    "\n",
    "N = sparkHC_pd.shape[0]\n",
    "\n",
    "FScore_sum = 0\n",
    "for meshid in true_classes_unique: # For each class \n",
    "    N_c = (sparkHC_pd.mesh_ui_diab == meshid).sum() #true_classes_documents.count(c)\n",
    "    #print(\"\\nc: {}, N_c: {}, N: {}, N_c/N : {}\".format(c, N_c, N, N_c/N))\n",
    "    temp_max_F1 = 0\n",
    "    for current_cluster in range(0, 32): # 21 # For each cluster\n",
    "        docs_for_current_cluster = sparkHC_pd[sparkHC_pd[\"prediction\"] == current_cluster]\n",
    "        class_count = docs_for_current_cluster[docs_for_current_cluster[\"mesh_ui_diab\"] == meshid].shape[0]#node.true_classes.count(c)\n",
    "        prec =  class_count / docs_for_current_cluster.shape[0] #node.counts\n",
    "        recall = class_count / N_c\n",
    "        #print(\"\\t cluster: {} :: class_count: {}, self.count: {}, p: {:.2f}, r: {:.2f}\".format(current_cluster, class_count, docs_for_current_cluster.shape[0], prec, recall))\n",
    "        if prec > 1e-10 or recall > 1e-10: # if prec or recall == 0 \n",
    "            F1 = 2 * prec * recall / (prec+recall)\n",
    "        else:\n",
    "            F1 = 0\n",
    "\n",
    "        if F1 > temp_max_F1:\n",
    "            temp_max_F1 = F1        \n",
    "            #print(\"\\t\\tJIP\")\n",
    "    #print(\"Best F1: {}\".format(temp_max_F1))\n",
    "    #print(\"Nc/N + F1 : {}\".format((N_c / N ) * temp_max_F1))\n",
    "    FScore_sum += (N_c / N ) * temp_max_F1\n",
    "    \n",
    "FScore_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(true_classes_unique)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Hi Francisco,\n",
    "so to give you an update on the performances.\n",
    "I changed the F1 score as we said taking the hierarchy of the classes into consideration. \n",
    "This said I had to remove abstracts which had as class the root class \"Diabetes Mellitus\", otherwise the F1 score\n",
    "would go over 1, I had this case. Also it seems logical, as we only have diabetes abstracts and want to cluster\n",
    "them in subcategories, we don't need the root class as all abstracts should be in the root. \n",
    "So the dataset was reduced from 55000 to 50000 abstracts by taking out the abstracts of the root class.\n",
    "That is why I had to rerun quite some simulations.\n",
    "\n",
    "So the performances on all tweets, only the F1_zhao score. In this case I checked the score for several\n",
    "cluster sizes. The number of leafs is always the same than the number of clusters \n",
    "\n",
    "##########\n",
    "ALL TWEETS, F1_Zhao scores\n",
    "\n",
    "FBE \n",
    "K64 => leafs 64 : 0.6839356043600647\n",
    "K32 => leafs 32 : 0.6837704144246328\n",
    "K20 => leafs 20 : 0.6836459020792343\n",
    "    \n",
    "scikitlearn\n",
    "K64 => leafs 64 : 0.6325404570785932\n",
    "K32 => leafs 32 : 0.6319947764196064\n",
    "K20 => leafs 20 : 0.6319261108094082\n",
    "\n",
    "    \n",
    "Here all tweets but the performance is only evaluated on the leafs, so we could compare with spark\n",
    "##########\n",
    "ALL TWEETS, F1_Zhao scores only leafs K=32\n",
    "\n",
    "FBE \n",
    "K64 => leafs 64 : 0.47186540814593\n",
    "K32 => leafs 32 : 0.4923950663139429\n",
    "K20 => leafs 20 : 0.4976217764396761\n",
    "    \n",
    "spark\n",
    "K64 : 0.28628270701459085\n",
    "K32 : 0.4646152670187616\n",
    "K20 : 0.49505558398827937\n",
    "\n",
    "scikitlearn\n",
    "K64 : 0.6307555786658395\n",
    "K32 : 0.6315189572838107\n",
    "K20 : 0.6315643667948513\n",
    "\n",
    "#######\n",
    "Now the optimisations for FBE. Again the number of leafs should be the same than the MAX_LEAFS,\n",
    "which is not always the case, sometimes FBE stops creating children\n",
    "\n",
    "# BASELINE\n",
    "Topwords 6; N = 5000 , N leafs: 32, MAX_LEAFS = 32\n",
    "'F1_zhao': 0.6591743123046493\n",
    "\n",
    "    \n",
    "# TOPWORDS\n",
    "Topwords 4; N = 5000, N_leafs 10, MAX_LEAFS = 32\n",
    "'F1_zhao': 0.6548679412028475\n",
    "\n",
    "Topwords 8; N = 5000, N leafs: 32, MAX_LEAFS = 32\n",
    "'F1_zhao': 0.7375354163826922\n",
    "\n",
    "Topwords 10; N = 5000, N leafs: 32, MAX_LEAFS = 32\n",
    "'F1_zhao': 0.6163431147102084\n",
    "\n",
    "\n",
    "# Parallelism 1\n",
    "Topwords 6; N = 5000 ; parallelism 1, N leafs: 32, MAX_LEAFS = 32\n",
    "'F1_zhao': 0.6591743123046493\n",
    "\n",
    "    \n",
    "# Shuffle + repartition (data...orderBy(rand()).repartition(parallelism))\n",
    "Topwords 6; N = 5000 ; N leafs: 32, MAX_LEAFS: 32, Shuffle+repartition  \n",
    "'F1_zhao': 0.6500299953630889\n",
    "\n",
    "\n",
    "# affect only class if higher score (HERE FBE only created 19 leafs, that is why the score is so much higher)\n",
    "Topwords 6; N = 5000 ; N leafs: 19, MAX_LEAFS: 32, affect class only if higher score\n",
    "'F1_zhao': 0.7382670537548556\n",
    "    \n",
    "    # that is why i took the baseline and created only 16 leafs in both cases and it seems that the optimisation improves performance\n",
    "    Topwords 6; N = 5000 ; N leafs: 16, MAX_LEAFS: 16\n",
    "    F1 zhao: 0.6121491555899314\n",
    "    Topwords 6; N = 5000 ; N leafs: 16, MAX_LEAFS: 16, affect class only if higher score\n",
    "    F1 zhao: 0.6840435816852406\n",
    "    \n",
    "\n",
    "To sum up:\n",
    "- On all tweets FBE had a gain of 5% performance over sklearn \n",
    "- On all tweets when evaluating only on the leafs FBE looses significantly performance whereas sklearn almost does not\n",
    "- Changing the topwords to 8 may improve performance\n",
    "- parallelism 1 is the same performance than the baseline (parallelism 3)\n",
    "- shuffle+repartition did not improve \n",
    "- it seems that affecting the class to only the tokens with highest score improves the performance. \n",
    "  But I need to figure out why FBE stops creating children at one point. This is not normal.\n",
    "    \n",
    "Tomorrow I took the day for holidays. Monday I would go continue to check out why FBE stops creating children."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
